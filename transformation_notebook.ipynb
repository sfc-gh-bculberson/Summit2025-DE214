{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hcx5k74wqpgxhaf6ywv4",
   "authorId": "503328055951",
   "authorName": "ADMIN",
   "authorEmail": "keith.gaputis@snowflake.com",
   "sessionId": "e41c1dfc-1178-48f1-9d84-e5241555aa2d",
   "lastEditTime": 1748812730375
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4cc6148-da07-4d4d-a9ec-9ce8d1f5c158",
   "metadata": {
    "name": "MD_INTRODUCTION_OBJECTIVE",
    "collapsed": false
   },
   "source": "# Streaming Data Pipeline with Snowpark Python and Dynamic Tables\n\n## Objective\nThis notebook demonstrates an enhanced approach to building a real-time analytics pipeline using Snowflake Dynamic Tables, Snowpark Python procedures, and Triggered Tasks. It focuses on transforming raw streaming ski resort data into actionable insights, with improved daily visit tracking and a structured aggregation hierarchy."
  },
  {
   "cell_type": "markdown",
   "id": "5f66c774-983f-450b-8f33-fc9582d2723d",
   "metadata": {
    "name": "MD_SETUP_INITIALIZATION",
    "collapsed": false
   },
   "source": "## 1. Setup and Initialization\n\nPython includes and initialize Snowpark environment"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "PY_SETUP_IMPORTS_SESSION"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nfrom snowflake.core import Root\n\n# Grab active Snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Initialize Snowflake Python API for object management\nroot = Root(session)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e3c449-8c6a-4e1e-a561-f25ba3d2bae4",
   "metadata": {
    "name": "MD_INITIAL_DATA_EXPLORATION"
   },
   "source": "## 2. Initial Data Exploration\n\nBefore building transformations, let's examine the structure of our raw streaming data. This helps in understanding the source tables we'll be working with."
  },
  {
   "cell_type": "code",
   "id": "a573b0ee-99d0-4e2f-96cf-c388507ff4f8",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_LIFT_RIDES_RAW"
   },
   "outputs": [],
   "source": "-- Lift usage events (core activity data)\nSELECT * FROM LIFT_RIDE LIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "767dc377-ff6b-40e9-8611-591786813faa",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_TICKETS_RAW"
   },
   "outputs": [],
   "source": "-- Day ticket purchases\nSELECT * FROM RESORT_TICKET LIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "229d7106-6412-4b79-9a84-2e4fbef92be4",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_PASSES_RAW"
   },
   "outputs": [],
   "source": "-- Season pass purchases\nSELECT * FROM SEASON_PASS LIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c781b2b4-73e9-4b5c-af5f-535ace340925",
   "metadata": {
    "name": "MD_DATA_INGESTION_STAGING_SETUP",
    "collapsed": false
   },
   "source": "## 3. Initial Data Pipeline Setup\n\nThis section covers the additional setup required for this use case, including creating streams and reference tables."
  },
  {
   "cell_type": "markdown",
   "id": "cb49eb8b-0b3a-45b6-a74a-d11bbdd12715",
   "metadata": {
    "name": "MD_LIFT_RIDE_STREAM"
   },
   "source": "### 3.1. Create Stream on Raw Lift Ride Data\n\nA stream is created on the `LIFT_RIDE` table to capture new lift ride events. This stream will be the source for the Snowpark procedure that populates daily visit information."
  },
  {
   "cell_type": "code",
   "id": "5dc661c0-e435-4454-82d8-5da5362bed96",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_LIFT_RIDE_STREAM"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STREAM LIFT_RIDE_STREAM ON TABLE LIFT_RIDE APPEND_ONLY = TRUE SHOW_INITIAL_ROWS = TRUE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f06db9b2-6ea2-4c93-a74e-384de53f8503",
   "metadata": {
    "name": "MD_REFERENCE_DATA_SETUP"
   },
   "source": "### 3.2. Resort Capacity Reference Table\n\nCreate and populate a reference table for resort capacities, which will be used in downstream calculations."
  },
  {
   "cell_type": "code",
   "id": "68c6e7f0-fbf0-49b6-8224-eb15a77fdacf",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_RESORT_CAPACITY_TABLE"
   },
   "outputs": [],
   "source": "-- Reference table for resort capacity\nCREATE OR REPLACE TABLE RESORT_CAPACITY (\n    RESORT VARCHAR(100) PRIMARY KEY,\n    MAX_CAPACITY INTEGER,\n    HOURLY_CAPACITY INTEGER,\n    BASE_LIFT_COUNT INTEGER,\n    IANA_TIMEZONE VARCHAR(50) \n);\n\nINSERT INTO RESORT_CAPACITY (RESORT, MAX_CAPACITY, HOURLY_CAPACITY, BASE_LIFT_COUNT, IANA_TIMEZONE) VALUES\n('Vail', 7000, 1100, 34, 'America/Denver'),\n('Beaver Creek', 5500, 900, 25, 'America/Denver'),\n('Breckenridge', 6500, 1000, 35, 'America/Denver'),\n('Keystone', 4500, 700, 21, 'America/Denver'),\n('Heavenly', 5000, 800, 27, 'America/Los_Angeles');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd15c6e9-f932-409e-9814-a31d7b348b41",
   "metadata": {
    "name": "MD_DAILY_VISIT_PROCESSING_SETUP"
   },
   "source": "## 4. Automated Daily Visit Processing with Snowpark\n\nThis section details the setup for accurately tracking daily visits using a Snowpark Stored Procedure and a Task to automate its execution."
  },
  {
   "cell_type": "markdown",
   "id": "3208eac2-49fb-403c-b761-a0b33e7b9bd6",
   "metadata": {
    "name": "MD_DAILY_VISITS_TABLE_SETUP"
   },
   "source": "### 4.1. `DAILY_VISITS` Table\n\nThis table will store unique daily visits per RFID at each resort, along with their first ride details and season pass status. It is populated by a Snowpark procedure."
  },
  {
   "cell_type": "code",
   "id": "b2920436-7ab4-4677-89c4-b2e3ebe1cd25",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_DAILY_VISITS_TABLE"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DAILY_VISITS (\n    VISIT_DATE DATE,\n    RESORT STRING,\n    RFID STRING,\n    NAME STRING,\n    FIRST_RIDE_TIME DATETIME,\n    FIRST_LIFT STRING,\n    HAS_SEASON_PASS BOOLEAN,\n    PURCHASE_PRICE_USD DECIMAL(7,2),    \n    ACTIVATION_USAGE_COUNT INTEGER,\n    TICKET_ORIGINAL_DURATION INTEGER\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "559350cc-ea18-4def-8898-ee21c2d491f2",
   "metadata": {
    "name": "MD_SNOWPARK_STAGE_SETUP",
    "collapsed": false
   },
   "source": "### 4.2. Stage for Deployed Snowpark Code\n\nCreate a stage to store Snowpark Python code for stored procedures."
  },
  {
   "cell_type": "code",
   "id": "74f55bf3-c9de-405f-b6d2-1b545f3ef7e8",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_SNOWPARK_APPS_STAGE"
   },
   "outputs": [],
   "source": "create stage if not exists snowpark_apps;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f6c9b38-6432-4a14-9bad-2f74e5e55147",
   "metadata": {
    "name": "MD_SNOWPARK_SPROC_DEFINITION",
    "collapsed": false
   },
   "source": "### 4.3. Snowpark Python Function: `populate_daily_visits`\n\nThis Python function will ultimately be deployed as a Python Stored Procedure. It processes new records from `LIFT_RIDE_STREAM`, identifies the first ride for each visitor per day at each resort, enriches the data with customer details and pass status, and inserts new, unique daily visits into the `DAILY_VISITS` table."
  },
  {
   "cell_type": "code",
   "id": "9de73cea-7cd4-40ff-8b9e-b852766ea349",
   "metadata": {
    "language": "python",
    "name": "PY_SPROC_POPULATE_DAILY_VISITS_DEF"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col, row_number, coalesce, when\nfrom snowflake.snowpark.window import Window\n\ndef populate_daily_visits(session: Session) -> str:\n    \"\"\"\n    Populate DAILY_VISITS table using Snowpark Python\n    Handles data from any date in the stream, deduplicates by RFID per resort per day\n    This process is designed to be run frequently from a triggered task\n    \"\"\"\n    \n    # Step 1: Get new rides from stream\n    lift_ride_stream = session.table(\"LIFT_RIDE_STREAM\")\n    \n    # Deduplicate by RFID per resort per day - get earliest ride time\n    window_spec = Window.partition_by(\n        col(\"RESORT\"), \n        col(\"RFID\"), \n        col(\"VISIT_DATE\")\n    ).order_by(col(\"RIDE_TIME\").asc())\n    \n    first_rides_df = lift_ride_stream.select(\n        col(\"RESORT\"),\n        col(\"RFID\"),\n        col(\"LIFT\").alias(\"FIRST_LIFT\"),\n        col(\"RIDE_TIME\").alias(\"FIRST_RIDE_TIME\"),        \n        col(\"RIDE_TIME\").cast('DATE').alias(\"VISIT_DATE\"),\n        col(\"ACTIVATION_DAY_COUNT\").alias(\"ACTIVATION_USAGE_COUNT\"), # Ride data includes total number of days ticket or pass has been activated\n        row_number().over(window_spec).alias(\"rn\")\n    )\n    # Filter to only first ride of each day for each RFID at each resort\n    first_rides_df = first_rides_df.filter(col(\"rn\") == 1) #.drop(col(\"rn\"))\n    \n    # Step 2: Join with customer data to get customer details and determine visit type\n    season_pass_df = session.table(\"SEASON_PASS\")\n    resort_ticket_df = session.table(\"RESORT_TICKET\")\n    \n    # Left join with season pass\n    first_rides_df = first_rides_df.join(season_pass_df, col(\"RFID\") == col(\"RFID_PASS\"), \"left\", rsuffix=\"_PASS\")\n    \n    # Left join with resort ticket\n    first_rides_df = first_rides_df.join(resort_ticket_df, col(\"RFID\") == col(\"RFID_TICKET\"), \"left\", rsuffix=\"_TICKET\")\n     \n    first_rides_df = first_rides_df.select(\n        first_rides_df.col(\"RESORT\"),\n        first_rides_df.col(\"RFID\"),\n        first_rides_df.col(\"FIRST_LIFT\"),\n        first_rides_df.col(\"FIRST_RIDE_TIME\"),\n        first_rides_df.col(\"VISIT_DATE\"), \n        coalesce(season_pass_df.col(\"NAME\"), resort_ticket_df.col(\"NAME\")).alias(\"NAME\"), # Name on ticket or pass\n        when(season_pass_df.col(\"RFID\").is_not_null(), True).otherwise(False).alias(\"HAS_SEASON_PASS\"),\n        coalesce(season_pass_df.col(\"PRICE_USD\"), resort_ticket_df.col(\"PRICE_USD\")).alias(\"PURCHASE_PRICE_USD\"), # Price of ticket or pass        \n        first_rides_df.col(\"ACTIVATION_USAGE_COUNT\"),\n        resort_ticket_df.col(\"DAYS\").alias(\"TICKET_ORIGINAL_DURATION\") #Will be null for passes\n    )\n    \n    # Step 3: Anti-join with existing DAILY_VISITS\n    daily_visits_df = session.table(\"DAILY_VISITS\").select(\n            col(\"VISIT_DATE\"),\n            col(\"RESORT\"),\n            col(\"RFID\")\n    )\n    # Create the anti-join condition - check for any existing record for this RFID/resort/date combination\n    new_visits_df = first_rides_df.join(daily_visits_df, \n        ((col(\"VISIT_DATE\") == col(\"VISIT_DATE_DV\")) &\n        (col(\"RESORT\") == col(\"RESORT_DV\")) &\n        (col(\"RFID\") == col(\"RFID_DV\"))), \"left\", rsuffix=\"_DV\").filter(col(\"RESORT_DV\").is_null())  # Anti-join condition        \n    new_visits_df = new_visits_df.select(\n        first_rides_df.col(\"VISIT_DATE\"),\n        first_rides_df.col(\"RESORT\"),\n        first_rides_df.col(\"RFID\"),\n        first_rides_df.col(\"NAME\"),\n        first_rides_df.col(\"FIRST_RIDE_TIME\"),\n        first_rides_df.col(\"FIRST_LIFT\"),\n        first_rides_df.col(\"HAS_SEASON_PASS\"),\n        first_rides_df.col(\"PURCHASE_PRICE_USD\"),\n        first_rides_df.col(\"ACTIVATION_USAGE_COUNT\"),\n        first_rides_df.col(\"TICKET_ORIGINAL_DURATION\") \n    )\n    \n    # Step 4: Append new visits into DAILY_VISITS table\n    try:\n        # Write the data to the table\n        new_visits_df.write.mode(\"append\").save_as_table(\"DAILY_VISITS\", column_order=\"name\")        \n        return \"OK\"\n    except Exception as e:\n        return f\"ERROR: {str(e)}\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f855c1c-f395-4eba-b7e2-e8d8acb15733",
   "metadata": {
    "name": "MD_INITIAL_SPROC_CALL_NOTE",
    "collapsed": false
   },
   "source": "### 4.4. Manually invoke Python function (for testing/setup)\n\nPrior to deploying as a Snowflake task, let's run the Python function to make sure it's working properly. This step will backfill initial data if `SHOW_INITIAL_ROWS=TRUE` was used for the stream and it's the first run."
  },
  {
   "cell_type": "code",
   "id": "e237c69f-30e3-4a73-8760-a4aae830bc73",
   "metadata": {
    "language": "python",
    "name": "PY_CALL_POPULATE_DAILY_VISITS_SPROC"
   },
   "outputs": [],
   "source": "populate_daily_visits(session)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d488026c-ddbd-401f-bd51-47c54ecc19b1",
   "metadata": {
    "name": "MD_SNOWPARK_TASK_CREATION",
    "collapsed": false
   },
   "source": "### 4.5. Create Triggered Task to Automate `populate_daily_visits`\n\nDefine and create a Snowflake Triggered Task to automatically run `populate_daily_visits` as a Python stored procedure when new data arrives in the `LIFT_RIDE_STREAM`."
  },
  {
   "cell_type": "code",
   "id": "e4a5f137-d590-4a60-9130-823fa2123ff3",
   "metadata": {
    "language": "python",
    "name": "PY_CREATE_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "from snowflake.core.task import StoredProcedureCall, Task\n\npopulate_dv_task = Task(\n    \"populate_daily_visits\",\n    StoredProcedureCall(populate_daily_visits, stage_location=\"@snowpark_apps\"),\n    warehouse=\"STREAMING_INGEST\", \n    condition=\"SYSTEM$STREAM_HAS_DATA('lift_ride_stream')\",\n    allow_overlapping_execution=False\n)\npopulate_dv_task_res = root.databases['streaming_ingest'].schemas['streaming_ingest'].tasks[\"populate_daily_visits\"]\npopulate_dv_task_res.create_or_alter(populate_dv_task)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ce827b5-2d8a-4319-9ad4-59df35268a6d",
   "metadata": {
    "name": "MD_TASK_MANAGEMENT_OPERATIONS"
   },
   "source": "## 5. Task Management\n\nCommands to manage the `populate_daily_visits`, such as suspending, checking parameters, altering, and resuming."
  },
  {
   "cell_type": "code",
   "id": "4c083fba-cb11-4d69-99c0-03669cfd9c13",
   "metadata": {
    "language": "python",
    "name": "PY_GET_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "populate_dv_task_res = root.databases['streaming_ingest'].schemas['streaming_ingest'].tasks[\"populate_daily_visits\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f25af90d-2986-4f80-9ddf-3be5b56352e0",
   "metadata": {
    "language": "python",
    "name": "PY_SUSPEND_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "populate_dv_task_res.suspend()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7fe0740-ef6d-48da-88e3-abd928b02761",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_TASK_TRIGGER_INTERVAL_PARAM"
   },
   "outputs": [],
   "source": "SHOW PARAMETERS LIKE 'USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS' IN TASK populate_daily_visits;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5be40452-2b54-476f-8a2a-a14808024a18",
   "metadata": {
    "language": "sql",
    "name": "SQL_ALTER_TASK_MINIMUM_TRIGGER_INTERVAL"
   },
   "outputs": [],
   "source": "-- Note: USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS controls the minimum execution interval for triggered tasks.\n-- By setting to 10 seconds, the task will run with maximum frequency.\nALTER TASK populate_daily_visits SET USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS = 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f03adb7-b084-4f3c-bbdb-3e50ea13d61c",
   "metadata": {
    "language": "python",
    "name": "PY_RESUME_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "# Resume the task to start its execution based on the stream condition\n# Ensure populate_dv_task_ref is defined from the PY_SUSPEND_POPULATE_DAILY_VISITS_TASK cell\npopulate_dv_task_res.resume()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fa67ac7-acd7-4b62-98e7-a6c0ca1a4544",
   "metadata": {
    "language": "sql",
    "name": "SQL_DESCRIBE_TASK"
   },
   "outputs": [],
   "source": "describe task populate_daily_visits;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56ce838d-6b79-47cd-bb66-82aec68587b8",
   "metadata": {
    "name": "MD_DYNAMIC_TABLE_AGGREGATION_PIPELINE_INTRO"
   },
   "source": "## 6. Dynamic Table Aggregation Pipeline\n\nDefine a series of Dynamic Tables to perform hierarchical aggregations (hourly, daily, weekly) on the ski resort data. These tables will automatically refresh as new data arrives."
  },
  {
   "cell_type": "markdown",
   "id": "ff2ad286-8b14-4790-9b6b-76a8e6951c8c",
   "metadata": {
    "name": "MD_DT_HOURLY_AGGREGATES_SETUP",
    "collapsed": false
   },
   "source": "### 6.1. Define Hourly Aggregations using SQL\n\nThese Dynamic Tables provide the first level of aggregation, summarizing data on an hourly basis. Let's start by using SQL to define the DTs."
  },
  {
   "cell_type": "code",
   "id": "7c2260ef-f366-4c14-b519-a5bff81b5e3c",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_LIFT_ACTIVITY"
   },
   "outputs": [],
   "source": "-- Summarize hourly lift activity\n-- Currently requires one join to determine how many riders have season passes and pass vs ticket rides\nCREATE OR REPLACE DYNAMIC TABLE HOURLY_LIFT_ACTIVITY\nTARGET_LAG='downstream' --This table will not be queried directly, so we can use downstream lag\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = incremental\nAS\nSELECT\n    DATE(lr.RIDE_TIME) as RIDE_DATE,\n    HOUR(lr.RIDE_TIME) as RIDE_HOUR,\n    DATE_TRUNC('hour', lr.RIDE_TIME) as RIDE_HOUR_TIMESTAMP,\n    lr.RESORT,\n    COUNT(*) as TOTAL_RIDES,\n    COUNT(DISTINCT lr.RFID) as VISITOR_COUNT,\n    -- Use DAILY_VISITS to determine pass usage\n    COUNT(DISTINCT CASE WHEN dv.HAS_SEASON_PASS = TRUE THEN lr.RFID END) as ACTIVE_PASSES,\n    COUNT(CASE WHEN dv.HAS_SEASON_PASS = TRUE THEN 1 END) as PASS_RIDES\nFROM LIFT_RIDE lr\nLEFT JOIN DAILY_VISITS dv ON lr.RFID = dv.RFID\n    AND DATE(lr.RIDE_TIME) = dv.VISIT_DATE\n    AND lr.RESORT = dv.RESORT\nGROUP BY RIDE_DATE, RIDE_HOUR, RIDE_HOUR_TIMESTAMP, lr.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bceca17-7cd5-4c0b-9071-6548011dba57",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_HOURLY_LIFT_ACTIVITY"
   },
   "outputs": [],
   "source": "-- Let's see what the data looks like\nselect * from HOURLY_LIFT_ACTIVITY \norder by RIDE_HOUR_TIMESTAMP desc \nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94dcf50e-1fc1-498a-ae5e-a619d2377fd2",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_AMORTIZED_REVENUE"
   },
   "outputs": [],
   "source": "-- Determine hourly revenue based on the activation of tickets and season passes\n-- Revenue from multi day tickets and season passes is prorated based on inline business rules\n-- NOTE: This revenue does not include hourly sales of tickets or passes - we would need an additional DT for that\nCREATE OR REPLACE DYNAMIC TABLE HOURLY_AMORTIZED_REVENUE\nTARGET_LAG = 'downstream'\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    dv.VISIT_DATE AS RIDE_DATE,\n    HOUR(dv.FIRST_RIDE_TIME) AS RIDE_HOUR,\n    DATE_TRUNC('hour', dv.FIRST_RIDE_TIME) AS RIDE_HOUR_TIMESTAMP,\n    dv.RESORT,\n    SUM(CASE\n        WHEN NOT dv.HAS_SEASON_PASS -- It's a ticket\n        THEN (dv.PURCHASE_PRICE_USD / GREATEST(dv.TICKET_ORIGINAL_DURATION, 1)) -- Use ticket duration to prorate revenue\n        ELSE 0\n    END) AS RECOGNIZED_TICKET_REVENUE,\n\n    COUNT(DISTINCT CASE WHEN NOT dv.HAS_SEASON_PASS THEN dv.RFID END) AS TICKET_ACTIVATIONS,\n\n    SUM(CASE\n        WHEN dv.HAS_SEASON_PASS AND dv.ACTIVATION_USAGE_COUNT <= 20 -- Prorate pass revenue across first 20 pass activations\n        THEN (dv.PURCHASE_PRICE_USD / 20) -- Recognize pass fractional revenue \n        ELSE 0\n    END) AS RECOGNIZED_PASS_REVENUE,\n\n    COUNT(DISTINCT CASE WHEN dv.HAS_SEASON_PASS THEN dv.RFID END) AS PASS_ACTIVATIONS\nFROM DAILY_VISITS dv\nGROUP BY RIDE_HOUR_TIMESTAMP, dv.RESORT, dv.VISIT_DATE, RIDE_HOUR;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5c6bb48-f3af-4c02-bdbd-d18f5295bb1d",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_AMORTIZED_REVENUE"
   },
   "outputs": [],
   "source": "select * from HOURLY_AMORTIZED_REVENUE \norder by RIDE_HOUR_TIMESTAMP desc \nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b26a4fdd-db5c-4c0b-abfd-f17ea6f5679d",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "-- Now let's combine together the first two intermediate DTs into a new consolidated DT that is easy to query\nCREATE OR REPLACE DYNAMIC TABLE HOURLY_RESORT_SUMMARY\nTARGET_LAG = '1 minute'\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    activity.RIDE_DATE,\n    activity.RIDE_HOUR,\n    activity.RIDE_HOUR_TIMESTAMP,\n    activity.RESORT,\n    activity.VISITOR_COUNT,\n    activity.TOTAL_RIDES,\n    -- Ticktet and pass activation counts\n    revenue.TICKET_ACTIVATIONS,\n    revenue.PASS_ACTIVATIONS,\n    -- General activity metrics \n    activity.ACTIVE_PASSES,\n    activity.PASS_RIDES,\n    (activity.VISITOR_COUNT - activity.ACTIVE_PASSES) AS ACTIVE_TICKETS,\n    (activity.TOTAL_RIDES - activity.PASS_RIDES) AS TICKET_RIDES,\n    -- Recognized Revenue from HOURLY_AMORTIZED_REVENUE\n    COALESCE(revenue.RECOGNIZED_TICKET_REVENUE, 0) AS RECOGNIZED_TICKET_REVENUE,\n    COALESCE(revenue.RECOGNIZED_PASS_REVENUE, 0) AS RECOGNIZED_PASS_REVENUE,\n    -- New Total Recognized Revenue\n    (COALESCE(revenue.RECOGNIZED_TICKET_REVENUE, 0) + COALESCE(revenue.RECOGNIZED_PASS_REVENUE, 0)) AS TOTAL_RECOGNIZED_REVENUE,\n    -- Calculate capacity percentage\n    ROUND((activity.VISITOR_COUNT / rc.MAX_CAPACITY * 100), 1) AS CAPACITY_PCT,\n    -- Capacity status\n    CASE\n        WHEN (activity.VISITOR_COUNT / rc.MAX_CAPACITY * 100) > 90 THEN 'HIGH'\n        WHEN (activity.VISITOR_COUNT / rc.MAX_CAPACITY * 100) > 70 THEN 'MODERATE'\n        ELSE 'NORMAL'\n        END AS CAPACITY_STATUS\nFROM HOURLY_LIFT_ACTIVITY activity\n         LEFT JOIN HOURLY_AMORTIZED_REVENUE revenue\n                   ON activity.RIDE_DATE = revenue.RIDE_DATE\n                       AND activity.RIDE_HOUR = revenue.RIDE_HOUR\n                       AND activity.RESORT = revenue.RESORT\n         JOIN RESORT_CAPACITY rc ON activity.RESORT = rc.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b8cc1bb-2778-4e01-a9b3-a3d87c4f10ce",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_HOURLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "select * from HOURLY_RESORT_SUMMARY \nwhere resort = 'Vail'\norder by RIDE_DATE desc, RIDE_HOUR desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa952f36-3a3f-40fc-adee-3cc0cc55eb93",
   "metadata": {
    "name": "MD_DT_DAILY_AGGREGATES_SETUP",
    "collapsed": false
   },
   "source": "### 6.2. Define Daily Aggregations using Snowpark Python\n \nThis next Dynamic Table rolls up hourly data. It will be very efficient to refresh because its calculated solely off of aggregated data.\n\n***While defining dynamic tables in SQL is nice,  we can also use Python!***"
  },
  {
   "cell_type": "code",
   "id": "c00b2ee4-b13e-43ce-8587-6486cbc39119",
   "metadata": {
    "language": "python",
    "name": "PY_DT_DAILY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, max as max_, sum as sum_, avg, count, round as round_, lit\n\n# Read from the HOURLY_RESORT_SUMMARY dynamic table\nhourly_summary_df = session.table(\"HOURLY_RESORT_SUMMARY\")\n\n# Group by RIDE_DATE and RESORT, then aggregate\ndaily_summary_df = (hourly_summary_df.group_by(\n    col(\"RIDE_DATE\"), \n    col(\"RESORT\")\n).agg(\n    max_(col(\"VISITOR_COUNT\")).alias(\"PEAK_HOURLY_VISITORS\"), # Peak visitors in any single hour        \n    sum_(col(\"VISITOR_COUNT\")).alias(\"TOTAL_VISITOR_HOURS\"), # Sum of all visitor-hours    \n    sum_(col(\"TOTAL_RIDES\")).alias(\"TOTAL_RIDES\"), # Total rides across all hours    \n    # Revenue aggregations\n    sum_(col(\"RECOGNIZED_TICKET_REVENUE\")).alias(\"TOTAL_TICKET_REVENUE\"),\n    sum_(col(\"RECOGNIZED_PASS_REVENUE\")).alias(\"TOTAL_PASS_REVENUE\"),\n    sum_(col(\"TOTAL_RECOGNIZED_REVENUE\")).alias(\"TOTAL_REVENUE\"),\n    # Visitor activations\n    sum_(col(\"TICKET_ACTIVATIONS\")).alias(\"TOTAL_TICKET_ACTIVATIONS\"),\n    sum_(col(\"PASS_ACTIVATIONS\")).alias(\"TOTAL_PASS_ACTIVATIONS\"),\n    # Ride type breakdowns\n    sum_(col(\"PASS_RIDES\")).alias(\"TOTAL_PASS_RIDES\"),\n    sum_(col(\"TICKET_RIDES\")).alias(\"TOTAL_TICKET_RIDES\"),\n    # Capacity metrics\n    round_(avg(col(\"CAPACITY_PCT\")), 1).alias(\"AVG_CAPACITY_PCT\"),\n    max_(col(\"CAPACITY_PCT\")).alias(\"PEAK_CAPACITY_PCT\"),\n    # Operation hours (count of hourly records)\n    count(lit(1)).alias(\"OPERATION_HOURS\")\n)\n    # Calculate total visitors as sum of ticket and pass activations\n    .with_column(\"TOTAL_VISITORS\", col(\"TOTAL_TICKET_ACTIVATIONS\") + col(\"TOTAL_PASS_ACTIVATIONS\")))\n    \n# Now we can use the Snowpark dataframe to deploy the next DT! \ndaily_summary_df.create_or_replace_dynamic_table(\n    name=\"DAILY_RESORT_SUMMARY\",\n    warehouse=\"STREAMING_INGEST\",\n    lag=\"1 minute\",\n    refresh_mode=\"INCREMENTAL\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb3c4734-4c15-47b4-a62e-d103c26bb9a4",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_DAILY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "select * from DAILY_RESORT_SUMMARY \norder by RIDE_DATE desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c9bd052-aaa1-41ec-b0b7-3298fc7d188a",
   "metadata": {
    "name": "MD_DT_WEEKLY_AGGREGATES_SETUP",
    "collapsed": false
   },
   "source": "### 6.3. Define Weekly Aggregation using Python\n\nThis last Dynamic Table aggregates daily summaries to provide weekly insights.  It will also be very efficient to calculate because it is calculated entirely based on daily data."
  },
  {
   "cell_type": "code",
   "id": "5beea08b-ceaf-4282-8cfe-2dabcf1dc56c",
   "metadata": {
    "language": "python",
    "name": "PY_DT_WEEKLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, max as max_, sum as sum_, avg, round as round_, date_trunc, count_distinct\n\n# Read from the DAILY_RESORT_SUMMARY dynamic table\ndaily_summary_df = session.table(\"DAILY_RESORT_SUMMARY\")\n\n# Group by WEEK_START_DATE (derived from RIDE_DATE) and RESORT, then aggregate\nweekly_summary_df = (daily_summary_df.group_by(\n    date_trunc('week', col(\"RIDE_DATE\")).alias(\"WEEK_START_DATE\"),\n    col(\"RESORT\")\n).agg(\n    max_(col(\"TOTAL_VISITORS\")).alias(\"MAX_DAILY_UNIQUE_VISITORS\"), # Peak unique visitors on any single day in the week\n    round_(avg(col(\"TOTAL_VISITORS\")), 0).alias(\"AVG_DAILY_UNIQUE_VISITORS\"), # Average daily unique visitors\n    sum_(col(\"TOTAL_VISITORS\")).alias(\"WEEK_TOTAL_VISITORS\"), # Sum of daily unique visitors (visitor-days)\n    sum_(col(\"TOTAL_RIDES\")).alias(\"WEEK_TOTAL_RIDES\"),\n    sum_(col(\"TOTAL_PASS_RIDES\")).alias(\"WEEK_TOTAL_PASS_RIDES\"),\n    sum_(col(\"TOTAL_TICKET_RIDES\")).alias(\"WEEK_TOTAL_TICKET_RIDES\"),\n    sum_(col(\"TOTAL_TICKET_REVENUE\")).alias(\"WEEK_TOTAL_TICKET_REVENUE\"),\n    sum_(col(\"TOTAL_PASS_REVENUE\")).alias(\"WEEK_TOTAL_PASS_REVENUE\"),\n    sum_(col(\"TOTAL_REVENUE\")).alias(\"WEEK_TOTAL_REVENUE\"),\n    round_(avg(col(\"TOTAL_REVENUE\")), 0).alias(\"AVG_DAILY_REVENUE\"),\n    sum_(col(\"TOTAL_TICKET_ACTIVATIONS\")).alias(\"WEEK_TOTAL_TICKET_ACTIVATIONS\"),\n    sum_(col(\"TOTAL_PASS_ACTIVATIONS\")).alias(\"WEEK_TOTAL_PASS_ACTIVATIONS\"),\n    round_(avg(col(\"AVG_CAPACITY_PCT\")), 1).alias(\"AVG_WEEK_CAPACITY_PCT\"), # Average of the daily average capacities\n    max_(col(\"PEAK_CAPACITY_PCT\")).alias(\"WEEK_PEAK_CAPACITY_PCT\"), # Peak hourly capacity reached during the week\n    count_distinct(col(\"RIDE_DATE\")).alias(\"OPERATION_DAYS\") # Count of distinct days with operations in the week\n))\n\n# Again, we can use the Snowpark dataframe to deploy our last DT\nweekly_summary_df.create_or_replace_dynamic_table(\n    name=\"WEEKLY_RESORT_SUMMARY2\",\n    warehouse=\"STREAMING_INGEST\",\n    lag=\"1 minute\",\n    refresh_mode=\"INCREMENTAL\"\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "459123e9-b822-4868-b41c-0fdc87ece572",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_WEEKLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "select * from WEEKLY_RESORT_SUMMARY \norder by WEEK_START_DATE desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "070dc4ea-a35e-4b29-8c13-446e33142848",
   "metadata": {
    "name": "MD_ANALYTICAL_VIEWS_SETUP",
    "collapsed": false
   },
   "source": "## 7. Analytical Views for Reporting\n\nCreate views on top of base tables and/or dynamic tables for easier querying and dashboarding. Views can easily be defined using SQL or Snowpark."
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_DAILY_REVENUE_PERFORMANCE",
    "codeCollapsed": false
   },
   "source": "-- ========================================\n-- VIEW: V_DAILY_REVENUE_PERFORMANCE\n-- Daily revenue vs targets, derived from DAILY_RESORT_SUMMARY and RESORT_CAPACITY\n-- ========================================\nCREATE OR REPLACE VIEW V_DAILY_REVENUE_PERFORMANCE AS\nWITH daily_targets AS (\n    SELECT\n        RESORT,\n        (MAX_CAPACITY * 0.7 * 100) as REVENUE_TARGET_USD -- Example target: 70% of max capacity value, assuming $100 per visitor\n    FROM RESORT_CAPACITY\n)\nSELECT\n    d.RIDE_DATE,\n    d.RESORT,\n    d.TOTAL_REVENUE,\n    t.REVENUE_TARGET_USD,\n    CASE\n        WHEN t.REVENUE_TARGET_USD > 0 THEN ROUND((d.TOTAL_REVENUE / t.REVENUE_TARGET_USD * 100), 1)\n        ELSE NULL\n        END as REVENUE_TARGET_PCT,\n    CASE\n        WHEN d.TOTAL_REVENUE >= t.REVENUE_TARGET_USD THEN 'ABOVE_TARGET'\n        WHEN d.TOTAL_REVENUE >= t.REVENUE_TARGET_USD * 0.9 THEN 'NEAR_TARGET'\n        ELSE 'BELOW_TARGET'\n        END as PERFORMANCE_STATUS\nFROM DAILY_RESORT_SUMMARY d\n         JOIN daily_targets t ON d.RESORT = t.RESORT;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "83387ed7-764c-4700-8647-95f1dd82ce73",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_VIEW_DAILY_REVENUE_PERFORMANCE"
   },
   "outputs": [],
   "source": "select * from V_DAILY_REVENUE_PERFORMANCE order by RIDE_DATE DESC LIMIT 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d204a4a7-a078-4a57-bd32-6049b558590e",
   "metadata": {
    "language": "python",
    "name": "PY_VIEW_DAILY_NETWORK_METRICS"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col, sum as sum_, avg, round as round_, count_distinct\n\ndaily_summary_df = session.table(\"DAILY_RESORT_SUMMARY\")\n# Group by RIDE_DATE and aggregate\nv_daily_network_metrics_df = (daily_summary_df\n    .group_by(col(\"RIDE_DATE\"))\n    .agg(\n        sum_(col(\"TOTAL_VISITORS\")).alias(\"TOTAL_NETWORK_VISITORS\"),\n        sum_(col(\"TOTAL_REVENUE\")).alias(\"TOTAL_NETWORK_REVENUE\"),\n        round_(avg(col(\"AVG_CAPACITY_PCT\")), 1).alias(\"AVG_NETWORK_CAPACITY_PCT\"),\n        sum_(col(\"TOTAL_RIDES\")).alias(\"TOTAL_NETWORK_RIDES\"),\n        count_distinct(col(\"RESORT\")).alias(\"ACTIVE_RESORTS\")\n    ))\n\n# Create or replace the view using the Snowpark dataframe\nv_daily_network_metrics_df.create_or_replace_view(\"V_DAILY_NETWORK_METRICS\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7906f19d-88eb-4a4c-b584-3e96ee251a0a",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_VIEW_DAILY_NETWORK_METRICS"
   },
   "outputs": [],
   "source": "select * from V_DAILY_NETWORK_METRICS order by RIDE_DATE DESC LIMIT 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "35fa1bae-488f-46c2-90f0-e375701bdd1f",
   "metadata": {
    "name": "MD_ANALYTICAL_PROCEDURES_SETUP",
    "collapsed": false
   },
   "source": "## 9. Analytical Stored Procedures for Reporting\n\nIn some cases, obtaining analytical results requires more flexibility than a `VIEW` provides.  Here is an example of a tabular Python stored procedure for calculating lift performance for a single resort over the last 30 minutes of lift operations.  This query will directly query the `LIFT_RIDE` base table, which is common in many streaming use cases. \n\n**NOTE:** This logic could also be written as a SQL UDTF, or embedded directly in a dashboard, but this example demonstrates how to encapsulate dynamic Python query logic for reusability."
  },
  {
   "cell_type": "code",
   "id": "e6c4b4b1-53e9-4061-80a8-420e94c13495",
   "metadata": {
    "language": "python",
    "name": "PY_CREATE_LIFT_PEFORMANCE_PROCEDURE"
   },
   "outputs": [],
   "source": "# Required imports for Snowpark operations and types\nfrom snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col, lit, max, count, count_distinct, min, when, dateadd, datediff, round as snowpark_round, row_number, sproc\nfrom snowflake.snowpark.window import Window\nfrom snowflake.snowpark.types import StructType, StructField, StringType, LongType, TimestampType, DoubleType, IntegerType\n\n# Define the schema for the output table of the stored procedure\n# This must match the structure of the DataFrame being returned.\noutput_schema = StructType([\n    StructField(\"RESORT\", StringType(), nullable=False),\n    StructField(\"LIFT\", StringType(), nullable=False),\n    StructField(\"RIDES\", LongType(), nullable=False),\n    StructField(\"UNIQUE_VISITORS\", LongType(), nullable=False),\n    StructField(\"FIRST_ACTIVITY_TIME\", TimestampType(), nullable=True), # Can be null if no rides\n    StructField(\"LAST_ACTIVITY_TIME\", TimestampType(), nullable=True),  # Can be null if no rides\n    StructField(\"USAGE_RANK_IN_RESORT\", IntegerType(), nullable=False), # Ranks are integers\n    StructField(\"OVERALL_USAGE_RANK\", IntegerType(), nullable=False),   # Ranks are integers\n    StructField(\"RIDES_PER_HOUR\", DoubleType(), nullable=True)         # Can be null or decimal\n])\n\n# Use the @sproc decorator with a struct return type to register a tabular stored procedure\n# This is similar to using a SQL UDTF, except this approach provides full access to a Snowpark session\n# TODO: Also accept time range args, so this logic can be used to analyze other time periods\n@sproc(\n    name=\"get_resort_lift_performance\",\n    return_type=output_schema,\n    input_types=[StringType()],\n    packages=['snowflake-snowpark-python'],\n    is_permanent=True, # Creates a permanent stored procedure\n    replace=True,      # Allows replacing an existing SP with the same name\n    stage_location = \"@snowpark_apps\" \n)\ndef get_resort_lift_stats_sp(snowpark_session: Session, resort_name_input: str):\n    \"\"\"\n    Snowpark Stored Procedure to get lift ride statistics for a specific resort.\n\n    Args:\n        session: The Snowpark session object (implicitly provided).\n        resort_name_input: The name of the resort to filter by.\n\n    Returns:\n        A Snowpark DataFrame with the lift ride statistics, matching output_schema.\n    \"\"\"\n\n    # Reference the LIFT_RIDE table\n    lift_ride_df = snowpark_session.table(\"LIFT_RIDE\")\n\n    # Get last ride for resort\n    resort_last_ride_df = lift_ride_df.filter(col(\"RESORT\") == resort_name_input) \\\n                                      .agg(max(col(\"RIDE_TIME\")).alias(\"last_ride_time\"))\n\n    # Main query logic\n    # First filter lift_ride for the specific resort\n    lr_filtered_df = lift_ride_df.filter(col(\"RESORT\") == resort_name_input)\n\n    # Cross join with last ride data\n    joined_df = lr_filtered_df.join(resort_last_ride_df, how=\"cross\")\n\n    # Apply the time filter\n    # Ensure last_ride_time is not null before attempting dateadd\n    thirty_minutes_before_last_ride = dateadd(\"minute\", lit(-30), col(\"last_ride_time\"))\n    filtered_rides_df = joined_df.filter(\n        (col(\"last_ride_time\").is_not_null()) & # Ensure last_ride_time exists\n        (col(\"RIDE_TIME\") > thirty_minutes_before_last_ride)\n    )\n    # If there was no last ride time, filtered_rides_df will be empty.\n\n    # Group by and aggregate\n    pre_aggregated_df = filtered_rides_df.group_by(col(\"RESORT\"), col(\"LIFT\")) \\\n                                        .agg(\n                                            count(lit(1)).alias(\"RIDES\"),\n                                            count_distinct(col(\"RFID\")).alias(\"UNIQUE_VISITORS\"),\n                                            min(col(\"RIDE_TIME\")).alias(\"FIRST_ACTIVITY_TIME\"),\n                                            max(col(\"RIDE_TIME\")).alias(\"LAST_ACTIVITY_TIME\")\n                                        )\n\n    # Define window specifications for ranking based on the aggregated \"RIDES\"\n    window_resort = Window.partition_by(col(\"RESORT\")).order_by(col(\"RIDES\").desc())\n    window_overall = Window.order_by(col(\"RIDES\").desc())\n\n    # Apply window functions and calculate RIDES_PER_HOUR\n    # Ensure columns from pre_aggregated_df are used here\n    final_df = pre_aggregated_df.select(\n        col(\"RESORT\"),\n        col(\"LIFT\"),\n        col(\"RIDES\"),\n        col(\"UNIQUE_VISITORS\"),\n        col(\"FIRST_ACTIVITY_TIME\"),\n        col(\"LAST_ACTIVITY_TIME\"),\n        row_number().over(window_resort).alias(\"USAGE_RANK_IN_RESORT\"),\n        row_number().over(window_overall).alias(\"OVERALL_USAGE_RANK\"),\n        #If activity range is <1min of data, set rides_per_hour to null to avoid divide by zero\n        #Otherwise calculate rides per hour across activity range \n        when(datediff(\"minute\", col(\"FIRST_ACTIVITY_TIME\"), col(\"LAST_ACTIVITY_TIME\")) == 0, lit(None).cast(DoubleType()))\n        .otherwise(\n            snowpark_round(\n                col(\"RIDES\") / (datediff(\"minute\", col(\"FIRST_ACTIVITY_TIME\"), col(\"LAST_ACTIVITY_TIME\")) / 60.0),\n                1\n            )\n        ).alias(\"RIDES_PER_HOUR\")\n    )\n\n    # Ensure the DataFrame schema matches the defined output_schema, especially nullable properties and types\n    # Snowpark will try to map, but explicit casting or selection order helps.\n    # The select statement above should produce columns in the correct order and type.\n    # If any column might be missing due to no data, default values and schema alignment is needed.\n\n    return final_df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9080f2ee-6afc-48f9-aae9-ddba9eeca155",
   "metadata": {
    "language": "python",
    "name": "PY_QUERY_LIFT_PEFORMANCE_PROCEDURE"
   },
   "outputs": [],
   "source": "# Get top 10 lifts for Vail in the last 30 minutes\nsession.table_function('get_resort_lift_performance', lit('Vail'))\\\n                  .filter(col(\"USAGE_RANK_IN_RESORT\") <= 10)\\\n                  .order_by(col(\"USAGE_RANK_IN_RESORT\"))",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b32f2a2e-6c1e-4f99-93ed-8e031146ef04",
   "metadata": {
    "name": "MD_SCHEMA_VERIFICATION_COMMANDS",
    "collapsed": false
   },
   "source": "## 9. Schema Verification\n\nShow tables and views to verify the created objects."
  },
  {
   "cell_type": "code",
   "id": "7e50eff0-167e-4bcc-81c8-08a85b230954",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_BASE_TABLES"
   },
   "outputs": [],
   "source": "-- List base tables\nSHOW TABLES;\nSELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE \"is_dynamic\" = 'N';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c751954-96da-44ea-a539-789d2d8559c3",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_DYNAMIC_TABLES"
   },
   "outputs": [],
   "source": "-- List dynamic tables\nSHOW DYNAMIC TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5339cfb1-9c79-4fc7-a108-be7c53650e13",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_VIEWS_VERIFICATION"
   },
   "outputs": [],
   "source": "-- List views\nSHOW VIEWS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0194a757-c05e-4b0e-8dad-e66d07867136",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_PROCEDURES_VERIFICATION"
   },
   "outputs": [],
   "source": "--List procedures\nSHOW PROCEDURES;\nSELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE \"schema_name\" = 'STREAMING_INGEST';",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0622d836-05dc-4c0b-b3b2-c46b990cab9c",
   "metadata": {
    "name": "MD_DYNAMIC_TABLE_OBSERVABILITY_QUERIES"
   },
   "source": "## 10. Dynamic Table Observability\n\nMonitor the health, refresh history, and status of your Dynamic Tables."
  },
  {
   "cell_type": "code",
   "id": "c7661178-733c-49f5-bba0-e98dca82d63c",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_REFRESH_HISTORY_MONITORING"
   },
   "outputs": [],
   "source": "-- Check refresh history for performance monitoring\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(NAME_PREFIX => 'STREAMING_INGEST.STREAMING_INGEST.'))\nORDER BY refresh_start_time DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ae30208-baad-4b84-8365-283a628e0d83",
   "metadata": {
    "name": "MD_CONCLUSION_AND_NEXT_STEPS"
   },
   "source": "## 11. Conclusion and Next Steps\n\nThis notebook has established an end-to-end streaming data pipeline incorporating Snowpark for complex transformations (`DAILY_VISITS`) and a hierarchy of Dynamic Tables for efficient, incremental aggregations.\n\n**Key Features Implemented:**\n- Automated daily unique visitor tracking using a Snowpark procedure and Task.\n- Multi-level aggregation pipeline (Hourly → Daily → Weekly) using Dynamic Tables.\n- Analytical views for simplified reporting and dashboarding.\n- Observability queries for monitoring Dynamic Table performance and health.\n\n**Potential Next Steps:**\n- Build Streamlit applications or connect BI tools to these views and Dynamic Tables for visualization.\n- Extend the pipeline with more advanced analytics, such as anomaly detection or predictive modeling.\n- Implement alerting based on DT status or data quality checks."
  }
 ]
}