{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "oazxizr3gauywsxry7ox",
   "authorId": "503328055951",
   "authorName": "ADMIN",
   "authorEmail": "keith.gaputis@snowflake.com",
   "sessionId": "78f7623d-efc2-400a-9ccd-5d9498f3425e",
   "lastEditTime": 1748637229103
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4cc6148-da07-4d4d-a9ec-9ce8d1f5c158",
   "metadata": {
    "name": "MD_INTRODUCTION_OBJECTIVE",
    "collapsed": false
   },
   "source": "# Streaming Data Pipeline with Snowpark Python and Dynamic Tables\n\n## Objective\nThis notebook demonstrates an enhanced approach to building a real-time analytics pipeline using Snowflake Dynamic Tables, Snowpark Python procedures, and Triggered Tasks. It focuses on transforming raw streaming ski resort data into actionable insights, with improved daily visit tracking and a structured aggregation hierarchy."
  },
  {
   "cell_type": "markdown",
   "id": "5f66c774-983f-450b-8f33-fc9582d2723d",
   "metadata": {
    "name": "MD_SETUP_INITIALIZATION",
    "collapsed": false
   },
   "source": "## 1. Setup and Initialization\n\nPython includes and initialize Snowpark environment"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "PY_SETUP_IMPORTS_SESSION"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e3c449-8c6a-4e1e-a561-f25ba3d2bae4",
   "metadata": {
    "name": "MD_INITIAL_DATA_EXPLORATION"
   },
   "source": "## 2. Initial Data Exploration\n\nBefore building transformations, let's examine the structure of our raw streaming data. This helps in understanding the source tables we'll be working with."
  },
  {
   "cell_type": "code",
   "id": "a573b0ee-99d0-4e2f-96cf-c388507ff4f8",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_LIFT_RIDES_RAW"
   },
   "outputs": [],
   "source": "-- Lift usage events (core activity data)\nSELECT *\nFROM LIFT_RIDE \nLIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "767dc377-ff6b-40e9-8611-591786813faa",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_TICKETS_RAW"
   },
   "outputs": [],
   "source": "-- Day ticket purchases\nSELECT * \nFROM RESORT_TICKET \nLIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "229d7106-6412-4b79-9a84-2e4fbef92be4",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_PASSES_RAW"
   },
   "outputs": [],
   "source": "-- Season pass purchases\nSELECT *\nFROM SEASON_PASS \nLIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c781b2b4-73e9-4b5c-af5f-535ace340925",
   "metadata": {
    "name": "MD_DATA_INGESTION_STAGING_SETUP",
    "collapsed": false
   },
   "source": "## 3. Initial Data Pipeline Setup\n\nThis section covers the additional setup required for this use case, including creating streams and reference tables."
  },
  {
   "cell_type": "markdown",
   "id": "cb49eb8b-0b3a-45b6-a74a-d11bbdd12715",
   "metadata": {
    "name": "MD_LIFT_RIDE_STREAM"
   },
   "source": "### 3.1. Create Stream on Raw Lift Ride Data\n\nA stream is created on the `LIFT_RIDE` table to capture new lift ride events. This stream will be the source for the Snowpark procedure that populates daily visit information."
  },
  {
   "cell_type": "code",
   "id": "5dc661c0-e435-4454-82d8-5da5362bed96",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_LIFT_RIDE_STREAM"
   },
   "outputs": [],
   "source": "CREATE STREAM IF NOT EXISTS LIFT_RIDE_STREAM ON TABLE LIFT_RIDE APPEND_ONLY = TRUE SHOW_INITIAL_ROWS = TRUE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f06db9b2-6ea2-4c93-a74e-384de53f8503",
   "metadata": {
    "name": "MD_REFERENCE_DATA_SETUP"
   },
   "source": "### 3.2. Resort Capacity Reference Table\n\nCreate and populate a reference table for resort capacities, which will be used in downstream calculations."
  },
  {
   "cell_type": "code",
   "id": "68c6e7f0-fbf0-49b6-8224-eb15a77fdacf",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_RESORT_CAPACITY_TABLE"
   },
   "outputs": [],
   "source": "-- Reference table for resort capacity\nCREATE OR REPLACE TABLE RESORT_CAPACITY (\n    RESORT VARCHAR(100) PRIMARY KEY,\n    MAX_CAPACITY INTEGER,\n    HOURLY_CAPACITY INTEGER,\n    BASE_LIFT_COUNT INTEGER\n);\n\nINSERT INTO RESORT_CAPACITY VALUES\n('Vail', 7000, 1100, 34),\n('Beaver Creek', 5500, 900, 25),\n('Breckenridge', 6500, 1000, 35),\n('Keystone', 4500, 700, 21),\n('Heavenly', 5000, 800, 27);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd15c6e9-f932-409e-9814-a31d7b348b41",
   "metadata": {
    "name": "MD_DAILY_VISIT_PROCESSING_SETUP"
   },
   "source": "## 4. Automated Daily Visit Processing with Snowpark\n\nThis section details the setup for accurately tracking daily visits using a Snowpark Stored Procedure and a Task to automate its execution."
  },
  {
   "cell_type": "markdown",
   "id": "3208eac2-49fb-403c-b761-a0b33e7b9bd6",
   "metadata": {
    "name": "MD_DAILY_VISITS_TABLE_SETUP"
   },
   "source": "### 4.1. `DAILY_VISITS` Table\n\nThis table will store unique daily visits per RFID at each resort, along with their first ride details and season pass status. It is populated by a Snowpark procedure."
  },
  {
   "cell_type": "code",
   "id": "b2920436-7ab4-4677-89c4-b2e3ebe1cd25",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_DAILY_VISITS_TABLE"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DAILY_VISITS (\n    VISIT_DATE DATE,\n    RESORT STRING,\n    RFID STRING,\n    NAME STRING,\n    FIRST_RIDE_TIME DATETIME,\n    FIRST_LIFT STRING,\n    HAS_SEASON_PASS BOOLEAN\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "559350cc-ea18-4def-8898-ee21c2d491f2",
   "metadata": {
    "name": "MD_SNOWPARK_STAGE_SETUP",
    "collapsed": false
   },
   "source": "### 4.2. Stage for Deployed Snowpark Code\n\nCreate a stage to store Snowpark Python code for stored procedures."
  },
  {
   "cell_type": "code",
   "id": "74f55bf3-c9de-405f-b6d2-1b545f3ef7e8",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_SNOWPARK_APPS_STAGE"
   },
   "outputs": [],
   "source": "create stage if not exists snowpark_apps;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f6c9b38-6432-4a14-9bad-2f74e5e55147",
   "metadata": {
    "name": "MD_SNOWPARK_SPROC_DEFINITION",
    "collapsed": false
   },
   "source": "### 4.3. Snowpark Python Function: `populate_daily_visits`\n\nThis Python function will ultimately be deployed as a Python Stored Procedure. It processes new records from `LIFT_RIDE_STREAM`, identifies the first ride for each visitor per day at each resort, enriches the data with customer details and pass status, and inserts new, unique daily visits into the `DAILY_VISITS` table."
  },
  {
   "cell_type": "code",
   "id": "9de73cea-7cd4-40ff-8b9e-b852766ea349",
   "metadata": {
    "language": "python",
    "name": "PY_SPROC_POPULATE_DAILY_VISITS_DEF"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col, row_number, coalesce, when, to_date\nfrom snowflake.snowpark.window import Window\n\ndef populate_daily_visits(session: Session) -> str:\n    \"\"\"\n    Populate DAILY_VISITS table using Snowpark Python\n    Handles data from any date in the stream, deduplicates by RFID per resort per day\n    This process is designed to be run frequently from a triggered task\n    \"\"\"\n    \n    # Step 1: Get new rides from stream\n    lift_ride_stream = session.table(\"LIFT_RIDE_STREAM\")\n    \n    # Deduplicate by RFID per resort per day - get earliest ride time\n    window_spec = Window.partition_by(\n        col(\"RESORT\"), \n        col(\"RFID\"), \n        to_date(col(\"RIDE_TIME\")) # Ensure partitioning by date part only\n    ).order_by(col(\"RIDE_TIME\").asc())\n    \n    first_rides_df = lift_ride_stream.select(\n        col(\"RESORT\"),\n        col(\"RFID\"),\n        col(\"LIFT\").alias(\"FIRST_LIFT\"),\n        col(\"RIDE_TIME\").alias(\"FIRST_RIDE_TIME\"),        \n        to_date(col(\"RIDE_TIME\")).alias(\"VISIT_DATE\"), # Use to_date for VISIT_DATE       \n        row_number().over(window_spec).alias(\"rn\")\n    )\n    # Filter to only first ride of each day for each RFID at each resort\n    first_rides_df = first_rides_df.filter(col(\"rn\") == 1).drop(\"rn\") # Drop rn after filtering\n    \n    # Step 2: Join with customer data to get customer details and determine visit type\n    # Corrected alias for season_pass and resort_ticket RFID columns to avoid ambiguity\n    season_pass_df = session.table(\"SEASON_PASS\").select(col(\"RFID\").alias(\"RFID_PASS\"), col(\"NAME\").alias(\"NAME_PASS\"))\n    resort_ticket_df = session.table(\"RESORT_TICKET\").select(col(\"RFID\").alias(\"RFID_TICKET\"), col(\"NAME\").alias(\"NAME_TICKET\"))\n    \n    # Left join with season pass\n    first_rides_df = first_rides_df.join(season_pass_df, first_rides_df.col(\"RFID\") == season_pass_df.col(\"RFID_PASS\"), \"left\")\n    \n    # Left join with resort ticket\n    first_rides_df = first_rides_df.join(resort_ticket_df, first_rides_df.col(\"RFID\") == resort_ticket_df.col(\"RFID_TICKET\"), \"left\")\n     \n    first_rides_df = first_rides_df.select(\n        first_rides_df.col(\"RESORT\"),\n        first_rides_df.col(\"RFID\"),\n        first_rides_df.col(\"FIRST_LIFT\"),\n        first_rides_df.col(\"FIRST_RIDE_TIME\"),\n        first_rides_df.col(\"VISIT_DATE\"),\n        coalesce(col(\"NAME_PASS\"), col(\"NAME_TICKET\")).alias(\"NAME\"),\n        when(col(\"RFID_PASS\").is_not_null(), True).otherwise(False).alias(\"HAS_SEASON_PASS\")\n    )\n    \n    # Step 3: Anti-join with existing DAILY_VISITS to find new visits\n    daily_visits_existing_df = session.table(\"DAILY_VISITS\").select(\n            col(\"VISIT_DATE\").alias(\"VISIT_DATE_DV\"),\n            col(\"RESORT\").alias(\"RESORT_DV\"),\n            col(\"RFID\").alias(\"RFID_DV\")\n    )\n\n    # Join condition for anti-join\n    join_condition = (\n        (first_rides_df.VISIT_DATE == daily_visits_existing_df.VISIT_DATE_DV) &\n        (first_rides_df.RESORT == daily_visits_existing_df.RESORT_DV) &\n        (first_rides_df.RFID == daily_visits_existing_df.RFID_DV)\n    )\n    \n    new_visits_df = first_rides_df.join(\n        daily_visits_existing_df, \n        join_condition, \n        \"left_anti\"\n    )\n    \n    # Select columns for insertion, ensuring correct order and names as per DAILY_VISITS table\n    new_visits_to_insert_df = new_visits_df.select(\n        col(\"VISIT_DATE\"),\n        col(\"RESORT\"),\n        col(\"RFID\"),\n        col(\"NAME\"),\n        col(\"FIRST_RIDE_TIME\"),\n        col(\"FIRST_LIFT\"),\n        col(\"HAS_SEASON_PASS\")\n    )\n    \n    # Step 4: Append new visits into DAILY_VISITS table\n    try:\n        # Write the data to the table\n        if new_visits_to_insert_df.count() > 0: # Only write if there's new data\n            new_visits_to_insert_df.write.mode(\"append\").save_as_table(\"DAILY_VISITS\") # Use default column name mapping\n            return f\"OK: Appended {new_visits_to_insert_df.count()} new visits.\"\n        else:\n            return \"OK: No new visits to append.\"\n    except Exception as e:\n        return f\"ERROR: {str(e)}\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f855c1c-f395-4eba-b7e2-e8d8acb15733",
   "metadata": {
    "name": "MD_INITIAL_SPROC_CALL_NOTE",
    "collapsed": false
   },
   "source": "### 4.4. Manually invoke Python function (for testing/setup)\n\nPrior to deploying as a Snowflake task, let's run the Python function to make sure it's working properly. This step will backfill initial data if `SHOW_INITIAL_ROWS=TRUE` was used for the stream and it's the first run."
  },
  {
   "cell_type": "code",
   "id": "e237c69f-30e3-4a73-8760-a4aae830bc73",
   "metadata": {
    "language": "python",
    "name": "PY_CALL_POPULATE_DAILY_VISITS_SPROC"
   },
   "outputs": [],
   "source": "populate_daily_visits(session)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d488026c-ddbd-401f-bd51-47c54ecc19b1",
   "metadata": {
    "name": "MD_SNOWPARK_TASK_CREATION",
    "collapsed": false
   },
   "source": "### 4.5. Create Triggered Task to Automate `populate_daily_visits`\n\nDefine and create a Snowflake Triggered Task to automatically run `populate_daily_visits` as a Python stored procedure when new data arrives in the `LIFT_RIDE_STREAM`."
  },
  {
   "cell_type": "code",
   "id": "e4a5f137-d590-4a60-9130-823fa2123ff3",
   "metadata": {
    "language": "python",
    "name": "PY_CREATE_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "from snowflake.core import Root\nfrom snowflake.core.task import StoredProcedureCall, Task\nroot = Root(session)\npopulate_dv_task = Task(\n    \"populate_daily_visits\",\n    StoredProcedureCall(populate_daily_visits, stage_location=\"@snowpark_apps\"),\n    warehouse=\"STREAMING_INGEST\", \n    condition=\"SYSTEM$STREAM_HAS_DATA('lift_ride_stream')\",\n    allow_overlapping_execution=False\n)\npopulate_dv_task_res = root.databases['streaming_ingest'].schemas['streaming_ingest'].tasks[\"populate_daily_visits\"]\npopulate_dv_task_res.create_or_alter(populate_dv_task)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ce827b5-2d8a-4319-9ad4-59df35268a6d",
   "metadata": {
    "name": "MD_TASK_MANAGEMENT_OPERATIONS"
   },
   "source": "## 5. Task Management\n\nCommands to manage the `populate_daily_visits`, such as suspending, checking parameters, altering, and resuming."
  },
  {
   "cell_type": "code",
   "id": "f25af90d-2986-4f80-9ddf-3be5b56352e0",
   "metadata": {
    "language": "python",
    "name": "PY_SUSPEND_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "populate_dv_task_res.suspend()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7fe0740-ef6d-48da-88e3-abd928b02761",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_TASK_TRIGGER_INTERVAL_PARAM"
   },
   "outputs": [],
   "source": "SHOW PARAMETERS LIKE 'USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS' IN TASK populate_daily_visits;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5be40452-2b54-476f-8a2a-a14808024a18",
   "metadata": {
    "language": "sql",
    "name": "SQL_ALTER_TASK_MINIMUM_TRIGGER_INTERVAL"
   },
   "outputs": [],
   "source": "-- Note: USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS controls the minimum execution interval for triggered tasks.\n-- By setting to 10 seconds, the task will run with maximum frequency.\nALTER TASK populate_daily_visits SET USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS = 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f03adb7-b084-4f3c-bbdb-3e50ea13d61c",
   "metadata": {
    "language": "python",
    "name": "PY_RESUME_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "# Resume the task to start its execution based on the stream condition\n# Ensure populate_dv_task_ref is defined from the PY_SUSPEND_POPULATE_DAILY_VISITS_TASK cell\npopulate_dv_task_res.resume()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56ce838d-6b79-47cd-bb66-82aec68587b8",
   "metadata": {
    "name": "MD_DYNAMIC_TABLE_AGGREGATION_PIPELINE_INTRO"
   },
   "source": "## 6. Dynamic Table Aggregation Pipeline\n\nDefine a series of Dynamic Tables to perform hierarchical aggregations (hourly, daily, weekly) on the ski resort data. These tables will automatically refresh as new data arrives."
  },
  {
   "cell_type": "markdown",
   "id": "ff2ad286-8b14-4790-9b6b-76a8e6951c8c",
   "metadata": {
    "name": "MD_DT_HOURLY_AGGREGATES_SETUP"
   },
   "source": "### 6.1. Hourly Aggregations\n\nThese Dynamic Tables provide the first level of aggregation, summarizing data on an hourly basis."
  },
  {
   "cell_type": "code",
   "id": "7c2260ef-f366-4c14-b519-a5bff81b5e3c",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_LIFT_ACTIVITY"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_LIFT_ACTIVITY \nTARGET_LAG='1 minute' \nWAREHOUSE = STREAMING_INGEST AS\nSELECT \n    DATE(lr.RIDE_TIME) as RIDE_DATE,\n    HOUR(lr.RIDE_TIME) as RIDE_HOUR,\n    lr.RESORT,\n    COUNT(*) as TOTAL_RIDES,\n    COUNT(DISTINCT lr.RFID) as VISITOR_COUNT,\n    -- Use DAILY_VISITS to determine pass usage for visitors active in this hour\n    COUNT(DISTINCT CASE WHEN dv.HAS_SEASON_PASS = TRUE THEN lr.RFID END) as ACTIVE_PASSES,\n    COUNT(CASE WHEN dv.HAS_SEASON_PASS = TRUE THEN 1 END) as PASS_RIDES\nFROM LIFT_RIDE lr\nLEFT JOIN DAILY_VISITS dv ON lr.RFID = dv.RFID \n    AND DATE(lr.RIDE_TIME) = dv.VISIT_DATE\n    AND lr.RESORT = dv.RESORT\nGROUP BY DATE(lr.RIDE_TIME), HOUR(lr.RIDE_TIME), lr.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a580dd7-d2c6-4d5b-badf-ea9f6cd56107",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_LIFT_TICKET_SALES"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_LIFT_TICKET_SALES\nTARGET_LAG='1 minute' \nWAREHOUSE = STREAMING_INGEST AS\nSELECT \n    DATE(PURCHASE_TIME) as PURCHASE_DATE,\n    HOUR(PURCHASE_TIME) as PURCHASE_HOUR,\n    RESORT,\n    SUM(PRICE_USD) as TICKET_REVENUE,\n    COUNT(*) as TICKETS_SOLD\nFROM RESORT_TICKET \nGROUP BY DATE(PURCHASE_TIME), HOUR(PURCHASE_TIME), RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b26a4fdd-db5c-4c0b-abfd-f17ea6f5679d",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_RESORT_SUMMARY\nTARGET_LAG='1 minute' -- Can be downstream if HOURLY_LIFT_ACTIVITY and HOURLY_LIFT_TICKET_SALES have shorter, fixed lags\nWAREHOUSE = STREAMING_INGEST AS\nSELECT \n    h.RIDE_DATE,\n    h.RIDE_HOUR,\n    h.RESORT,\n    h.VISITOR_COUNT,\n    h.TOTAL_RIDES,\n    COALESCE(t.TICKET_REVENUE, 0) as TICKET_REVENUE,\n    COALESCE(t.TICKETS_SOLD, 0) as TICKETS_SOLD,\n    h.ACTIVE_PASSES,\n    h.PASS_RIDES,\n    -- Calculate capacity percentage\n    ROUND((h.VISITOR_COUNT / rc.MAX_CAPACITY * 100), 1) as CAPACITY_PCT,\n    -- Calculate total revenue (tickets + allocated pass value: $5.50 per active pass use in the hour)\n    COALESCE(t.TICKET_REVENUE, 0) + (h.PASS_RIDES * 5.50) as TOTAL_REVENUE, -- Changed from ACTIVE_PASSES to PASS_RIDES for revenue calc\n    -- Capacity status\n    CASE \n        WHEN (h.VISITOR_COUNT / rc.MAX_CAPACITY * 100) > 90 THEN 'HIGH'\n        WHEN (h.VISITOR_COUNT / rc.MAX_CAPACITY * 100) > 70 THEN 'MODERATE'\n        ELSE 'NORMAL'\n    END as CAPACITY_STATUS\nFROM HOURLY_LIFT_ACTIVITY h\nLEFT JOIN HOURLY_LIFT_TICKET_SALES t \n    ON h.RIDE_DATE = t.PURCHASE_DATE \n    AND h.RIDE_HOUR = t.PURCHASE_HOUR \n    AND h.RESORT = t.RESORT\nJOIN RESORT_CAPACITY rc ON h.RESORT = rc.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa952f36-3a3f-40fc-adee-3cc0cc55eb93",
   "metadata": {
    "name": "MD_DT_DAILY_AGGREGATES_SETUP",
    "collapsed": false
   },
   "source": "### 6.2. Daily Aggregations\n\nThis Dynamic Table rolls up hourly data and incorporates accurate daily visitor counts from the `DAILY_VISITS` table."
  },
  {
   "cell_type": "code",
   "id": "f057a333-174d-47e1-ab61-7a017f5d10d0",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_DAILY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "-- ========================================\n-- DAILY RESORT SUMMARY DYNAMIC TABLE\n-- Daily aggregation from hourly DT + accurate visitor counts from DAILY_VISITS\n-- ========================================\nCREATE OR REPLACE DYNAMIC TABLE DAILY_RESORT_SUMMARY\nTARGET_LAG='1 minute' -- Can be 'downstream' if HOURLY_RESORT_SUMMARY has a fixed lag\nWAREHOUSE = STREAMING_INGEST AS\nWITH daily_hourly_aggregation AS (\n    SELECT \n        RIDE_DATE,\n        RESORT,\n        MAX(VISITOR_COUNT) as PEAK_HOURLY_VISITORS, -- Renamed for clarity\n        SUM(VISITOR_COUNT) as TOTAL_VISITOR_HOURS,\n        SUM(TOTAL_RIDES) as TOTAL_RIDES,\n        SUM(TOTAL_REVENUE) as TOTAL_REVENUE,\n        SUM(TICKET_REVENUE) as TICKET_REVENUE,\n        SUM(TICKETS_SOLD) as TICKETS_SOLD,\n        SUM(PASS_RIDES) as TOTAL_PASS_RIDES, -- Changed from ACTIVE_PASSES\n        ROUND(AVG(CAPACITY_PCT), 1) as AVG_CAPACITY_PCT,\n        MAX(CAPACITY_PCT) as PEAK_CAPACITY_PCT,\n        COUNT(*) as OPERATION_HOURS\n    FROM HOURLY_RESORT_SUMMARY\n    GROUP BY RIDE_DATE, RESORT\n),\ndaily_visitor_count AS (\n    SELECT \n        VISIT_DATE,\n        RESORT,\n        COUNT(DISTINCT RFID) as TOTAL_UNIQUE_VISITORS -- Renamed for clarity\n    FROM DAILY_VISITS\n    GROUP BY VISIT_DATE, RESORT\n)\nSELECT \n    h.RIDE_DATE,\n    h.RESORT,\n    COALESCE(v.TOTAL_UNIQUE_VISITORS, 0) as TOTAL_VISITORS, -- This is the accurate daily unique visitor count\n    h.PEAK_HOURLY_VISITORS,\n    h.TOTAL_VISITOR_HOURS,\n    h.TOTAL_RIDES,\n    h.TOTAL_REVENUE,\n    h.TICKET_REVENUE,\n    h.TICKETS_SOLD,\n    h.TOTAL_PASS_RIDES, -- Renamed from TOTAL_PASS_USES\n    h.AVG_CAPACITY_PCT,\n    h.PEAK_CAPACITY_PCT,\n    h.OPERATION_HOURS\nFROM daily_hourly_aggregation h\nLEFT JOIN daily_visitor_count v \n    ON h.RIDE_DATE = v.VISIT_DATE \n    AND h.RESORT = v.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c9bd052-aaa1-41ec-b0b7-3298fc7d188a",
   "metadata": {
    "name": "MD_DT_WEEKLY_AGGREGATES_SETUP"
   },
   "source": "### 6.3. Weekly Aggregations\n\nThis Dynamic Table aggregates daily summaries to provide weekly insights."
  },
  {
   "cell_type": "code",
   "id": "a2675295-9549-45c0-b12d-8280c916a94d",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_WEEKLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "-- ========================================\n-- WEEKLY RESORT SUMMARY DYNAMIC TABLE\n-- Weekly aggregation from daily DT\n-- ========================================\nCREATE OR REPLACE DYNAMIC TABLE WEEKLY_RESORT_SUMMARY\nTARGET_LAG='1 minute' -- Can be 'downstream' if DAILY_RESORT_SUMMARY has a fixed lag\nWAREHOUSE = STREAMING_INGEST AS\nSELECT \n    DATE_TRUNC('week', RIDE_DATE) as WEEK_START_DATE,\n    RESORT,\n    MAX(PEAK_HOURLY_VISITORS) as WEEK_PEAK_HOURLY_VISITORS, -- Reflects peak hourly within the week\n    ROUND(AVG(TOTAL_VISITORS), 0) as AVG_DAILY_UNIQUE_VISITORS, -- Avg of unique daily visitors\n    SUM(TOTAL_RIDES) as WEEK_TOTAL_RIDES,\n    SUM(TOTAL_REVENUE) as WEEK_TOTAL_REVENUE,\n    ROUND(AVG(TOTAL_REVENUE), 0) as AVG_DAILY_REVENUE,\n    SUM(TICKETS_SOLD) as WEEK_TICKETS_SOLD,\n    SUM(TOTAL_PASS_RIDES) as WEEK_TOTAL_PASS_RIDES, -- Reflects sum of daily pass rides\n    ROUND(AVG(AVG_CAPACITY_PCT), 1) as AVG_WEEK_CAPACITY_PCT,\n    MAX(PEAK_CAPACITY_PCT) as WEEK_PEAK_CAPACITY_PCT,\n    COUNT(DISTINCT RIDE_DATE) as OPERATION_DAYS -- Count of distinct days with operations\nFROM DAILY_RESORT_SUMMARY\nGROUP BY DATE_TRUNC('week', RIDE_DATE), RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "070dc4ea-a35e-4b29-8c13-446e33142848",
   "metadata": {
    "name": "MD_ANALYTICAL_VIEWS_SETUP",
    "collapsed": false
   },
   "source": "## 7. Analytical Views for Reporting\n\nCreate views on top of base tables and/or dynamic tables for easier querying and dashboarding."
  },
  {
   "cell_type": "code",
   "id": "5c851e66-e5fe-4e08-9100-043acfc0923f",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_RT_LIFT_PERFORMANCE"
   },
   "outputs": [],
   "source": "-- ========================================\n-- VIEW: V_RT_LIFT_PERFORMANCE\n-- Real-time lift performance based on last 30 minutes of activity from LIFT_RIDE table\n-- ========================================\nCREATE OR REPLACE VIEW V_RT_LIFT_PERFORMANCE AS\nWITH simulation_clock AS (\n    -- Determine the latest ride time to simulate a 'current time' for the batch data\n    SELECT         \n        MAX(RIDE_TIME) as MAX_RIDE_TIME\n    FROM LIFT_RIDE     \n),\nrecent_activity AS (\n    SELECT \n        lr.RESORT,\n        lr.LIFT,\n        COUNT(*) as RIDES_30MIN,\n        COUNT(DISTINCT lr.RFID) as VISITORS_30MIN,\n        MAX(lr.RIDE_TIME) as LAST_ACTIVITY_TIME\n    FROM LIFT_RIDE lr\n    CROSS JOIN simulation_clock clock -- Use CROSS JOIN if clock returns one row, or ensure appropriate join condition\n    WHERE lr.RIDE_TIME >= DATEADD(MINUTE, -30, clock.MAX_RIDE_TIME) AND lr.RIDE_TIME <= clock.MAX_RIDE_TIME\n    GROUP BY lr.RESORT, lr.LIFT\n    HAVING COUNT(*) > 0 -- Ensure there was activity\n)\nSELECT \n    ra.RESORT,\n    ra.LIFT,\n    ra.RIDES_30MIN,\n    ra.VISITORS_30MIN,\n    ra.LAST_ACTIVITY_TIME,\n    ROUND(ra.RIDES_30MIN * 2.0, 1) as ESTIMATED_RIDES_PER_HOUR, -- Simplified: rides in 30 mins * 2\n    ROW_NUMBER() OVER (PARTITION BY ra.RESORT ORDER BY ra.RIDES_30MIN DESC) as USAGE_RANK_IN_RESORT\nFROM recent_activity ra\nORDER BY ra.RESORT, ra.RIDES_30MIN DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_DAILY_REVENUE_PERFORMANCE"
   },
   "source": "-- ========================================\n-- VIEW: V_DAILY_REVENUE_PERFORMANCE\n-- Daily revenue vs targets, derived from DAILY_RESORT_SUMMARY and RESORT_CAPACITY\n-- ========================================\nCREATE OR REPLACE VIEW V_DAILY_REVENUE_PERFORMANCE AS\nWITH daily_targets AS (\n    SELECT \n        RESORT,\n        (MAX_CAPACITY * 0.7 * 100) as REVENUE_TARGET_USD -- Example target: 70% of max capacity value, assuming $100 per visitor\n    FROM RESORT_CAPACITY\n)\nSELECT \n    d.RIDE_DATE,\n    d.RESORT,\n    d.TOTAL_REVENUE,\n    t.REVENUE_TARGET_USD,\n    CASE \n        WHEN t.REVENUE_TARGET_USD > 0 THEN ROUND((d.TOTAL_REVENUE / t.REVENUE_TARGET_USD * 100), 1)\n        ELSE NULL \n    END as REVENUE_TARGET_PCT,\n    CASE \n        WHEN d.TOTAL_REVENUE >= t.REVENUE_TARGET_USD THEN 'ABOVE_TARGET'\n        WHEN d.TOTAL_REVENUE >= t.REVENUE_TARGET_USD * 0.9 THEN 'NEAR_TARGET'\n        ELSE 'BELOW_TARGET'\n    END as PERFORMANCE_STATUS\nFROM DAILY_RESORT_SUMMARY d\nJOIN daily_targets t ON d.RESORT = t.RESORT;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "d204a4a7-a078-4a57-bd32-6049b558590e",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_DAILY_NETWORK_METRICS"
   },
   "outputs": [],
   "source": "-- ========================================\n-- VIEW: V_DAILY_NETWORK_METRICS\n-- Simplified network-wide metrics for dashboard, derived from DAILY_RESORT_SUMMARY\n-- ========================================\nCREATE OR REPLACE VIEW V_DAILY_NETWORK_METRICS AS\nSELECT \n    RIDE_DATE,\n    SUM(TOTAL_VISITORS) as TOTAL_NETWORK_VISITORS,\n    SUM(TOTAL_REVENUE) as TOTAL_NETWORK_REVENUE,\n    ROUND(AVG(AVG_CAPACITY_PCT), 1) as AVG_NETWORK_CAPACITY_PCT, -- Average of average daily capacities\n    SUM(TOTAL_RIDES) as TOTAL_NETWORK_RIDES,\n    COUNT(DISTINCT RESORT) as ACTIVE_RESORTS\nFROM DAILY_RESORT_SUMMARY\nGROUP BY RIDE_DATE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b32f2a2e-6c1e-4f99-93ed-8e031146ef04",
   "metadata": {
    "name": "MD_SCHEMA_VERIFICATION_COMMANDS"
   },
   "source": "## 9. Schema Verification\n\nShow tables and views to verify the created objects."
  },
  {
   "cell_type": "code",
   "id": "7c751954-96da-44ea-a539-789d2d8559c3",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_TABLES_AND_DYNAMIC_TABLES"
   },
   "outputs": [],
   "source": "SHOW DYNAMIC TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5339cfb1-9c79-4fc7-a108-be7c53650e13",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_VIEWS_VERIFICATION"
   },
   "outputs": [],
   "source": "SHOW VIEWS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0622d836-05dc-4c0b-b3b2-c46b990cab9c",
   "metadata": {
    "name": "MD_DYNAMIC_TABLE_OBSERVABILITY_QUERIES"
   },
   "source": "## 10. Dynamic Table Observability\n\nMonitor the health, refresh history, and status of your Dynamic Tables."
  },
  {
   "cell_type": "code",
   "id": "c7661178-733c-49f5-bba0-e98dca82d63c",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_REFRESH_HISTORY_MONITORING"
   },
   "outputs": [],
   "source": "-- Check refresh history for performance monitoring\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(NAME_PREFIX => 'STREAMING_INGEST.STREAMING_INGEST.'))\nORDER BY refresh_start_time DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ae30208-baad-4b84-8365-283a628e0d83",
   "metadata": {
    "name": "MD_CONCLUSION_AND_NEXT_STEPS"
   },
   "source": "## 11. Conclusion and Next Steps\n\nThis notebook has established an end-to-end streaming data pipeline incorporating Snowpark for complex transformations (`DAILY_VISITS`) and a hierarchy of Dynamic Tables for efficient, incremental aggregations.\n\n**Key Features Implemented:**\n- Automated daily unique visitor tracking using a Snowpark procedure and Task.\n- Multi-level aggregation pipeline (Hourly → Daily → Weekly) using Dynamic Tables.\n- Analytical views for simplified reporting and dashboarding.\n- Observability queries for monitoring Dynamic Table performance and health.\n\n**Potential Next Steps:**\n- Build Streamlit applications or connect BI tools to these views and Dynamic Tables for visualization.\n- Extend the pipeline with more advanced analytics, such as anomaly detection or predictive modeling.\n- Implement alerting based on DT status or data quality checks."
  }
 ]
}
