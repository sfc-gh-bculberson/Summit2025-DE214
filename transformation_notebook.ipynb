{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hcx5k74wqpgxhaf6ywv4",
   "authorId": "503328055951",
   "authorName": "ADMIN",
   "authorEmail": "keith.gaputis@snowflake.com",
   "sessionId": "5ed4cf09-708e-4bea-9f36-b5bbac435e2f",
   "lastEditTime": 1748725756598
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4cc6148-da07-4d4d-a9ec-9ce8d1f5c158",
   "metadata": {
    "name": "MD_INTRODUCTION_OBJECTIVE",
    "collapsed": false
   },
   "source": "# Streaming Data Pipeline with Snowpark Python and Dynamic Tables\n\n## Objective\nThis notebook demonstrates an enhanced approach to building a real-time analytics pipeline using Snowflake Dynamic Tables, Snowpark Python procedures, and Triggered Tasks. It focuses on transforming raw streaming ski resort data into actionable insights, with improved daily visit tracking and a structured aggregation hierarchy."
  },
  {
   "cell_type": "markdown",
   "id": "5f66c774-983f-450b-8f33-fc9582d2723d",
   "metadata": {
    "name": "MD_SETUP_INITIALIZATION",
    "collapsed": false
   },
   "source": "## 1. Setup and Initialization\n\nPython includes and initialize Snowpark environment"
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "PY_SETUP_IMPORTS_SESSION"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\nfrom snowflake.core import Root\n\n# Grab active Snowpark session\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\n# Initialize Snowflake Python API for object management\nroot = Root(session)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d7e3c449-8c6a-4e1e-a561-f25ba3d2bae4",
   "metadata": {
    "name": "MD_INITIAL_DATA_EXPLORATION"
   },
   "source": "## 2. Initial Data Exploration\n\nBefore building transformations, let's examine the structure of our raw streaming data. This helps in understanding the source tables we'll be working with."
  },
  {
   "cell_type": "code",
   "id": "a573b0ee-99d0-4e2f-96cf-c388507ff4f8",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_LIFT_RIDES_RAW"
   },
   "outputs": [],
   "source": "-- Lift usage events (core activity data)\nSELECT * FROM LIFT_RIDE LIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "767dc377-ff6b-40e9-8611-591786813faa",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_TICKETS_RAW"
   },
   "outputs": [],
   "source": "-- Day ticket purchases\nSELECT * FROM RESORT_TICKET LIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "229d7106-6412-4b79-9a84-2e4fbef92be4",
   "metadata": {
    "language": "sql",
    "name": "SQL_EXPLORE_PASSES_RAW"
   },
   "outputs": [],
   "source": "-- Season pass purchases\nSELECT * FROM SEASON_PASS LIMIT 20;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c781b2b4-73e9-4b5c-af5f-535ace340925",
   "metadata": {
    "name": "MD_DATA_INGESTION_STAGING_SETUP",
    "collapsed": false
   },
   "source": "## 3. Initial Data Pipeline Setup\n\nThis section covers the additional setup required for this use case, including creating streams and reference tables."
  },
  {
   "cell_type": "markdown",
   "id": "cb49eb8b-0b3a-45b6-a74a-d11bbdd12715",
   "metadata": {
    "name": "MD_LIFT_RIDE_STREAM"
   },
   "source": "### 3.1. Create Stream on Raw Lift Ride Data\n\nA stream is created on the `LIFT_RIDE` table to capture new lift ride events. This stream will be the source for the Snowpark procedure that populates daily visit information."
  },
  {
   "cell_type": "code",
   "id": "5dc661c0-e435-4454-82d8-5da5362bed96",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_LIFT_RIDE_STREAM"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE STREAM LIFT_RIDE_STREAM ON TABLE LIFT_RIDE APPEND_ONLY = TRUE SHOW_INITIAL_ROWS = TRUE;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f06db9b2-6ea2-4c93-a74e-384de53f8503",
   "metadata": {
    "name": "MD_REFERENCE_DATA_SETUP"
   },
   "source": "### 3.2. Resort Capacity Reference Table\n\nCreate and populate a reference table for resort capacities, which will be used in downstream calculations."
  },
  {
   "cell_type": "code",
   "id": "68c6e7f0-fbf0-49b6-8224-eb15a77fdacf",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_RESORT_CAPACITY_TABLE"
   },
   "outputs": [],
   "source": "-- Reference table for resort capacity\nCREATE OR REPLACE TABLE RESORT_CAPACITY (\n    RESORT VARCHAR(100) PRIMARY KEY,\n    MAX_CAPACITY INTEGER,\n    HOURLY_CAPACITY INTEGER,\n    BASE_LIFT_COUNT INTEGER,\n    IANA_TIMEZONE VARCHAR(50) \n);\n\nINSERT INTO RESORT_CAPACITY (RESORT, MAX_CAPACITY, HOURLY_CAPACITY, BASE_LIFT_COUNT, IANA_TIMEZONE) VALUES\n('Vail', 7000, 1100, 34, 'America/Denver'),\n('Beaver Creek', 5500, 900, 25, 'America/Denver'),\n('Breckenridge', 6500, 1000, 35, 'America/Denver'),\n('Keystone', 4500, 700, 21, 'America/Denver'),\n('Heavenly', 5000, 800, 27, 'America/Los_Angeles');",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cd15c6e9-f932-409e-9814-a31d7b348b41",
   "metadata": {
    "name": "MD_DAILY_VISIT_PROCESSING_SETUP"
   },
   "source": "## 4. Automated Daily Visit Processing with Snowpark\n\nThis section details the setup for accurately tracking daily visits using a Snowpark Stored Procedure and a Task to automate its execution."
  },
  {
   "cell_type": "markdown",
   "id": "3208eac2-49fb-403c-b761-a0b33e7b9bd6",
   "metadata": {
    "name": "MD_DAILY_VISITS_TABLE_SETUP"
   },
   "source": "### 4.1. `DAILY_VISITS` Table\n\nThis table will store unique daily visits per RFID at each resort, along with their first ride details and season pass status. It is populated by a Snowpark procedure."
  },
  {
   "cell_type": "code",
   "id": "b2920436-7ab4-4677-89c4-b2e3ebe1cd25",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_DAILY_VISITS_TABLE"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE TABLE DAILY_VISITS (\n    VISIT_DATE DATE,\n    RESORT STRING,\n    RFID STRING,\n    NAME STRING,\n    FIRST_RIDE_TIME DATETIME,\n    FIRST_LIFT STRING,\n    HAS_SEASON_PASS BOOLEAN,\n    PURCHASE_PRICE_USD DECIMAL(7,2),    \n    ACTIVATION_USAGE_COUNT INTEGER,\n    TICKET_ORIGINAL_DURATION INTEGER\n);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "559350cc-ea18-4def-8898-ee21c2d491f2",
   "metadata": {
    "name": "MD_SNOWPARK_STAGE_SETUP",
    "collapsed": false
   },
   "source": "### 4.2. Stage for Deployed Snowpark Code\n\nCreate a stage to store Snowpark Python code for stored procedures."
  },
  {
   "cell_type": "code",
   "id": "74f55bf3-c9de-405f-b6d2-1b545f3ef7e8",
   "metadata": {
    "language": "sql",
    "name": "SQL_CREATE_SNOWPARK_APPS_STAGE"
   },
   "outputs": [],
   "source": "create stage if not exists snowpark_apps;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f6c9b38-6432-4a14-9bad-2f74e5e55147",
   "metadata": {
    "name": "MD_SNOWPARK_SPROC_DEFINITION",
    "collapsed": false
   },
   "source": "### 4.3. Snowpark Python Function: `populate_daily_visits`\n\nThis Python function will ultimately be deployed as a Python Stored Procedure. It processes new records from `LIFT_RIDE_STREAM`, identifies the first ride for each visitor per day at each resort, enriches the data with customer details and pass status, and inserts new, unique daily visits into the `DAILY_VISITS` table."
  },
  {
   "cell_type": "code",
   "id": "9de73cea-7cd4-40ff-8b9e-b852766ea349",
   "metadata": {
    "language": "python",
    "name": "PY_SPROC_POPULATE_DAILY_VISITS_DEF"
   },
   "outputs": [],
   "source": "from snowflake.snowpark import Session\nfrom snowflake.snowpark.functions import col, row_number, coalesce, when\nfrom snowflake.snowpark.window import Window\n\ndef populate_daily_visits(session: Session) -> str:\n    \"\"\"\n    Populate DAILY_VISITS table using Snowpark Python\n    Handles data from any date in the stream, deduplicates by RFID per resort per day\n    This process is designed to be run frequently from a triggered task\n    \"\"\"\n    \n    # Step 1: Get new rides from stream\n    lift_ride_stream = session.table(\"LIFT_RIDE_STREAM\")\n    \n    # Deduplicate by RFID per resort per day - get earliest ride time\n    window_spec = Window.partition_by(\n        col(\"RESORT\"), \n        col(\"RFID\"), \n        col(\"VISIT_DATE\")\n    ).order_by(col(\"RIDE_TIME\").asc())\n    \n    first_rides_df = lift_ride_stream.select(\n        col(\"RESORT\"),\n        col(\"RFID\"),\n        col(\"LIFT\").alias(\"FIRST_LIFT\"),\n        col(\"RIDE_TIME\").alias(\"FIRST_RIDE_TIME\"),        \n        col(\"RIDE_TIME\").cast('DATE').alias(\"VISIT_DATE\"),\n        col(\"ACTIVATION_DAY_COUNT\").alias(\"ACTIVATION_USAGE_COUNT\"), # Ride data includes total number of days ticket or pass has been activated\n        row_number().over(window_spec).alias(\"rn\")\n    )\n    # Filter to only first ride of each day for each RFID at each resort\n    first_rides_df = first_rides_df.filter(col(\"rn\") == 1) #.drop(col(\"rn\"))\n    \n    # Step 2: Join with customer data to get customer details and determine visit type\n    season_pass_df = session.table(\"SEASON_PASS\")\n    resort_ticket_df = session.table(\"RESORT_TICKET\")\n    \n    # Left join with season pass\n    first_rides_df = first_rides_df.join(season_pass_df, col(\"RFID\") == col(\"RFID_PASS\"), \"left\", rsuffix=\"_PASS\")\n    \n    # Left join with resort ticket\n    first_rides_df = first_rides_df.join(resort_ticket_df, col(\"RFID\") == col(\"RFID_TICKET\"), \"left\", rsuffix=\"_TICKET\")\n     \n    first_rides_df = first_rides_df.select(\n        first_rides_df.col(\"RESORT\"),\n        first_rides_df.col(\"RFID\"),\n        first_rides_df.col(\"FIRST_LIFT\"),\n        first_rides_df.col(\"FIRST_RIDE_TIME\"),\n        first_rides_df.col(\"VISIT_DATE\"), \n        coalesce(season_pass_df.col(\"NAME\"), resort_ticket_df.col(\"NAME\")).alias(\"NAME\"), # Name on ticket or pass\n        when(season_pass_df.col(\"RFID\").is_not_null(), True).otherwise(False).alias(\"HAS_SEASON_PASS\"),\n        coalesce(season_pass_df.col(\"PRICE_USD\"), resort_ticket_df.col(\"PRICE_USD\")).alias(\"PURCHASE_PRICE_USD\"), # Price of ticket or pass        \n        first_rides_df.col(\"ACTIVATION_USAGE_COUNT\"),\n        resort_ticket_df.col(\"DAYS\").alias(\"TICKET_ORIGINAL_DURATION\") #Will be null for passes\n    )\n    \n    # Step 3: Anti-join with existing DAILY_VISITS\n    daily_visits_df = session.table(\"DAILY_VISITS\").select(\n            col(\"VISIT_DATE\"),\n            col(\"RESORT\"),\n            col(\"RFID\")\n    )\n    # Create the anti-join condition - check for any existing record for this RFID/resort/date combination\n    new_visits_df = first_rides_df.join(daily_visits_df, \n        ((col(\"VISIT_DATE\") == col(\"VISIT_DATE_DV\")) &\n        (col(\"RESORT\") == col(\"RESORT_DV\")) &\n        (col(\"RFID\") == col(\"RFID_DV\"))), \"left\", rsuffix=\"_DV\").filter(col(\"RESORT_DV\").is_null())  # Anti-join condition        \n    new_visits_df = new_visits_df.select(\n        first_rides_df.col(\"VISIT_DATE\"),\n        first_rides_df.col(\"RESORT\"),\n        first_rides_df.col(\"RFID\"),\n        first_rides_df.col(\"NAME\"),\n        first_rides_df.col(\"FIRST_RIDE_TIME\"),\n        first_rides_df.col(\"FIRST_LIFT\"),\n        first_rides_df.col(\"HAS_SEASON_PASS\"),\n        first_rides_df.col(\"PURCHASE_PRICE_USD\"),\n        first_rides_df.col(\"ACTIVATION_USAGE_COUNT\"),\n        first_rides_df.col(\"TICKET_ORIGINAL_DURATION\") \n    )\n    \n    # Step 4: Append new visits into DAILY_VISITS table\n    try:\n        # Write the data to the table\n        new_visits_df.write.mode(\"append\").save_as_table(\"DAILY_VISITS\", column_order=\"name\")        \n        return \"OK\"\n    except Exception as e:\n        return f\"ERROR: {str(e)}\"",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0f855c1c-f395-4eba-b7e2-e8d8acb15733",
   "metadata": {
    "name": "MD_INITIAL_SPROC_CALL_NOTE",
    "collapsed": false
   },
   "source": "### 4.4. Manually invoke Python function (for testing/setup)\n\nPrior to deploying as a Snowflake task, let's run the Python function to make sure it's working properly. This step will backfill initial data if `SHOW_INITIAL_ROWS=TRUE` was used for the stream and it's the first run."
  },
  {
   "cell_type": "code",
   "id": "e237c69f-30e3-4a73-8760-a4aae830bc73",
   "metadata": {
    "language": "python",
    "name": "PY_CALL_POPULATE_DAILY_VISITS_SPROC"
   },
   "outputs": [],
   "source": "populate_daily_visits(session)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d488026c-ddbd-401f-bd51-47c54ecc19b1",
   "metadata": {
    "name": "MD_SNOWPARK_TASK_CREATION",
    "collapsed": false
   },
   "source": "### 4.5. Create Triggered Task to Automate `populate_daily_visits`\n\nDefine and create a Snowflake Triggered Task to automatically run `populate_daily_visits` as a Python stored procedure when new data arrives in the `LIFT_RIDE_STREAM`."
  },
  {
   "cell_type": "code",
   "id": "e4a5f137-d590-4a60-9130-823fa2123ff3",
   "metadata": {
    "language": "python",
    "name": "PY_CREATE_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "from snowflake.core.task import StoredProcedureCall, Task\n\npopulate_dv_task = Task(\n    \"populate_daily_visits\",\n    StoredProcedureCall(populate_daily_visits, stage_location=\"@snowpark_apps\"),\n    warehouse=\"STREAMING_INGEST\", \n    condition=\"SYSTEM$STREAM_HAS_DATA('lift_ride_stream')\",\n    allow_overlapping_execution=False\n)\npopulate_dv_task_res = root.databases['streaming_ingest'].schemas['streaming_ingest'].tasks[\"populate_daily_visits\"]\npopulate_dv_task_res.create_or_alter(populate_dv_task)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ce827b5-2d8a-4319-9ad4-59df35268a6d",
   "metadata": {
    "name": "MD_TASK_MANAGEMENT_OPERATIONS"
   },
   "source": "## 5. Task Management\n\nCommands to manage the `populate_daily_visits`, such as suspending, checking parameters, altering, and resuming."
  },
  {
   "cell_type": "code",
   "id": "4c083fba-cb11-4d69-99c0-03669cfd9c13",
   "metadata": {
    "language": "python",
    "name": "PY_GET_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "populate_dv_task_res = root.databases['streaming_ingest'].schemas['streaming_ingest'].tasks[\"populate_daily_visits\"]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f25af90d-2986-4f80-9ddf-3be5b56352e0",
   "metadata": {
    "language": "python",
    "name": "PY_SUSPEND_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "populate_dv_task_res.suspend()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d7fe0740-ef6d-48da-88e3-abd928b02761",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_TASK_TRIGGER_INTERVAL_PARAM"
   },
   "outputs": [],
   "source": "SHOW PARAMETERS LIKE 'USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS' IN TASK populate_daily_visits;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5be40452-2b54-476f-8a2a-a14808024a18",
   "metadata": {
    "language": "sql",
    "name": "SQL_ALTER_TASK_MINIMUM_TRIGGER_INTERVAL"
   },
   "outputs": [],
   "source": "-- Note: USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS controls the minimum execution interval for triggered tasks.\n-- By setting to 10 seconds, the task will run with maximum frequency.\nALTER TASK populate_daily_visits SET USER_TASK_MINIMUM_TRIGGER_INTERVAL_IN_SECONDS = 10;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5f03adb7-b084-4f3c-bbdb-3e50ea13d61c",
   "metadata": {
    "language": "python",
    "name": "PY_RESUME_POPULATE_DAILY_VISITS_TASK"
   },
   "outputs": [],
   "source": "# Resume the task to start its execution based on the stream condition\n# Ensure populate_dv_task_ref is defined from the PY_SUSPEND_POPULATE_DAILY_VISITS_TASK cell\npopulate_dv_task_res.resume()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fa67ac7-acd7-4b62-98e7-a6c0ca1a4544",
   "metadata": {
    "language": "sql",
    "name": "SQL_DESCRIBE_TASK"
   },
   "outputs": [],
   "source": "describe task populate_daily_visits;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "56ce838d-6b79-47cd-bb66-82aec68587b8",
   "metadata": {
    "name": "MD_DYNAMIC_TABLE_AGGREGATION_PIPELINE_INTRO"
   },
   "source": "## 6. Dynamic Table Aggregation Pipeline\n\nDefine a series of Dynamic Tables to perform hierarchical aggregations (hourly, daily, weekly) on the ski resort data. These tables will automatically refresh as new data arrives."
  },
  {
   "cell_type": "markdown",
   "id": "ff2ad286-8b14-4790-9b6b-76a8e6951c8c",
   "metadata": {
    "name": "MD_DT_HOURLY_AGGREGATES_SETUP"
   },
   "source": "### 6.1. Hourly Aggregations\n\nThese Dynamic Tables provide the first level of aggregation, summarizing data on an hourly basis."
  },
  {
   "cell_type": "code",
   "id": "7c2260ef-f366-4c14-b519-a5bff81b5e3c",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_LIFT_ACTIVITY"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_LIFT_ACTIVITY\nTARGET_LAG='1 minute'\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = incremental\nAS\nSELECT\n    DATE(lr.RIDE_TIME) as RIDE_DATE,\n    HOUR(lr.RIDE_TIME) as RIDE_HOUR,\n    DATE_TRUNC('hour', lr.RIDE_TIME) as RIDE_HOUR_TIMESTAMP,\n    lr.RESORT,\n    COUNT(*) as TOTAL_RIDES,\n    COUNT(DISTINCT lr.RFID) as VISITOR_COUNT,\n    -- Use DAILY_VISITS to determine pass usage\n    COUNT(DISTINCT CASE WHEN dv.HAS_SEASON_PASS = TRUE THEN lr.RFID END) as ACTIVE_PASSES,\n    COUNT(CASE WHEN dv.HAS_SEASON_PASS = TRUE THEN 1 END) as PASS_RIDES\n    -- COUNT(DISTINCT CASE\n    --     WHEN dv.HAS_SEASON_PASS = TRUE\n    --     AND HOUR(dv.FIRST_RIDE_TIME) = HOUR(lr.RIDE_TIME)\n    --     THEN lr.RFID\n    -- END) as PASSES_ACTIVATED,\n    -- COUNT(DISTINCT CASE\n    --     WHEN dv.HAS_SEASON_PASS = FALSE \n    --     AND HOUR(dv.FIRST_RIDE_TIME) = HOUR(lr.RIDE_TIME)\n    --     THEN dv.RFID\n    -- END) AS TICKETS_ACTIVATED \nFROM LIFT_RIDE lr\nLEFT JOIN DAILY_VISITS dv ON lr.RFID = dv.RFID\n    AND DATE(lr.RIDE_TIME) = dv.VISIT_DATE\n    AND lr.RESORT = dv.RESORT\nGROUP BY RIDE_DATE, RIDE_HOUR, RIDE_HOUR_TIMESTAMP, lr.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8bceca17-7cd5-4c0b-9071-6548011dba57",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_HOURLY_LIFT_ACTIVITY"
   },
   "outputs": [],
   "source": "select * from HOURLY_LIFT_ACTIVITY \norder by RIDE_HOUR_TIMESTAMP desc \nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94dcf50e-1fc1-498a-ae5e-a619d2377fd2",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_AMORTIZED_REVENUE"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_AMORTIZED_REVENUE\nTARGET_LAG = '1 minute'\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    dv.VISIT_DATE AS RIDE_DATE,\n    HOUR(dv.FIRST_RIDE_TIME) AS RIDE_HOUR,\n    DATE_TRUNC('hour', dv.FIRST_RIDE_TIME) AS RIDE_HOUR_TIMESTAMP,\n    dv.RESORT,\n    SUM(CASE\n        WHEN NOT dv.HAS_SEASON_PASS -- It's a ticket\n        THEN (dv.PURCHASE_PRICE_USD / GREATEST(dv.TICKET_ORIGINAL_DURATION, 1)) -- Use actual ticket duration\n        ELSE 0\n    END) AS RECOGNIZED_TICKET_REVENUE,\n\n    COUNT(DISTINCT CASE WHEN NOT dv.HAS_SEASON_PASS THEN dv.RFID END) AS TICKET_ACTIVATIONS,\n\n    SUM(CASE\n        WHEN dv.HAS_SEASON_PASS AND dv.ACTIVATION_USAGE_COUNT <= 20 -- Usage cap for passes\n        THEN (dv.PURCHASE_PRICE_USD / 20) -- Amortize pass price over a fixed number (e.g., 20 days)\n        ELSE 0\n    END) AS RECOGNIZED_PASS_REVENUE,\n\n    COUNT(DISTINCT CASE WHEN dv.HAS_SEASON_PASS THEN dv.RFID END) AS PASS_ACTIVATIONS\nFROM DAILY_VISITS dv\nGROUP BY RIDE_HOUR_TIMESTAMP, dv.RESORT, dv.VISIT_DATE, RIDE_HOUR;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d5c6bb48-f3af-4c02-bdbd-d18f5295bb1d",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_AMORTIZED_REVENUE"
   },
   "outputs": [],
   "source": "select * from HOURLY_AMORTIZED_REVENUE \norder by RIDE_HOUR_TIMESTAMP desc \nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9a580dd7-d2c6-4d5b-badf-ea9f6cd56107",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_LIFT_TICKET_SALES"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_LIFT_TICKET_SALES\nTARGET_LAG='1 minute' \nWAREHOUSE = STREAMING_INGEST \nREFRESH_MODE = incremental\nAS\nSELECT \n    DATE(PURCHASE_TIME) as PURCHASE_DATE,\n    HOUR(PURCHASE_TIME) as PURCHASE_HOUR,\n    RESORT,\n    SUM(PRICE_USD) as TICKET_REVENUE,\n    COUNT(*) as TICKETS_SOLD\nFROM RESORT_TICKET \nGROUP BY DATE(PURCHASE_TIME), HOUR(PURCHASE_TIME), RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d633bffb-ae3e-4b15-b9d0-7cc899441132",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_HOURLY_LIFT_TICKET_SALES"
   },
   "outputs": [],
   "source": "select * from HOURLY_LIFT_TICKET_SALES \norder by PURCHASE_DATE desc, PURCHASE_HOUR desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b26a4fdd-db5c-4c0b-abfd-f17ea6f5679d",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_HOURLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE DYNAMIC TABLE HOURLY_RESORT_SUMMARY\nTARGET_LAG = '1 minute'\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    hla.RIDE_DATE,\n    hla.RIDE_HOUR,\n    hla.RIDE_HOUR_TIMESTAMP,\n    hla.RESORT,\n    hla.VISITOR_COUNT,\n    hla.TOTAL_RIDES,\n\n    -- Ticktet and pass activation counts\n    har.TICKET_ACTIVATIONS, \n    har.PASS_ACTIVATIONS,  \n\n    -- General activity metrics \n    hla.ACTIVE_PASSES, \n    hla.PASS_RIDES,        \n    (hla.VISITOR_COUNT - hla.ACTIVE_PASSES) AS ACTIVE_TICKETS,\n    (hla.TOTAL_RIDES - hla.PASS_RIDES) AS TICKET_RIDES,\n    \n    -- Recognized Revenue from HOURLY_AMORTIZED_REVENUE\n    COALESCE(har.RECOGNIZED_TICKET_REVENUE, 0) AS RECOGNIZED_TICKET_REVENUE,\n    COALESCE(har.RECOGNIZED_PASS_REVENUE, 0) AS RECOGNIZED_PASS_REVENUE,\n\n    -- New Total Recognized Revenue\n    (COALESCE(har.RECOGNIZED_TICKET_REVENUE, 0) + COALESCE(har.RECOGNIZED_PASS_REVENUE, 0)) AS TOTAL_RECOGNIZED_REVENUE,\n\n    -- Calculate capacity percentage\n    ROUND((hla.VISITOR_COUNT / rc.MAX_CAPACITY * 100), 1) AS CAPACITY_PCT,\n\n    -- Capacity status\n    CASE\n        WHEN (hla.VISITOR_COUNT / rc.MAX_CAPACITY * 100) > 90 THEN 'HIGH'\n        WHEN (hla.VISITOR_COUNT / rc.MAX_CAPACITY * 100) > 70 THEN 'MODERATE'\n        ELSE 'NORMAL'\n    END AS CAPACITY_STATUS\n\nFROM HOURLY_LIFT_ACTIVITY hla\nLEFT JOIN HOURLY_AMORTIZED_REVENUE har\n    ON hla.RIDE_DATE = har.RIDE_DATE\n    AND hla.RIDE_HOUR = har.RIDE_HOUR\n    AND hla.RESORT = har.RESORT\nJOIN RESORT_CAPACITY rc ON hla.RESORT = rc.RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b8cc1bb-2778-4e01-a9b3-a3d87c4f10ce",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_HOURLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "select * from HOURLY_RESORT_SUMMARY \nwhere resort = 'Vail' and ride_date = '2026-08-10'\norder by RIDE_DATE desc, RIDE_HOUR desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa952f36-3a3f-40fc-adee-3cc0cc55eb93",
   "metadata": {
    "name": "MD_DT_DAILY_AGGREGATES_SETUP",
    "collapsed": false
   },
   "source": "### 6.2. Daily Aggregations\n\nThis Dynamic Table rolls up hourly data and incorporates accurate daily visitor counts from the `DAILY_VISITS` table."
  },
  {
   "cell_type": "code",
   "id": "f057a333-174d-47e1-ab61-7a017f5d10d0",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_DAILY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "-- ========================================\n-- DAILY RESORT SUMMARY DYNAMIC TABLE\n-- Daily aggregation from hourly DT + accurate visitor counts from DAILY_VISITS\n-- ========================================\nCREATE OR REPLACE DYNAMIC TABLE DAILY_RESORT_SUMMARY\nTARGET_LAG='1 minute' -- Or your desired lag, e.g., '60 minutes' if daily is less frequent\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    RIDE_DATE,\n    RESORT,\n    MAX(VISITOR_COUNT) AS PEAK_HOURLY_VISITORS,\n    SUM(VISITOR_COUNT) AS TOTAL_VISITOR_HOURS,\n    SUM(TOTAL_RIDES) AS TOTAL_RIDES,\n    SUM(RECOGNIZED_TICKET_REVENUE) AS TOTAL_TICKET_REVENUE,\n    SUM(RECOGNIZED_PASS_REVENUE) AS TOTAL_PASS_REVENUE,\n    SUM(TOTAL_RECOGNIZED_REVENUE) AS TOTAL_REVENUE,\n    SUM(TICKET_ACTIVATIONS) AS TOTAL_TICKET_ACTIVATIONS,\n    SUM(PASS_ACTIVATIONS) AS TOTAL_PASS_ACTIVATIONS,    \n    (TOTAL_TICKET_ACTIVATIONS + TOTAL_PASS_ACTIVATIONS) AS TOTAL_VISITORS,\n    SUM(PASS_RIDES) AS TOTAL_PASS_RIDES,\n    SUM(TICKET_RIDES) AS TOTAL_TICKET_RIDES,\n    ROUND(AVG(CAPACITY_PCT), 1) AS AVG_CAPACITY_PCT,\n    MAX(CAPACITY_PCT) AS PEAK_CAPACITY_PCT,\n    COUNT(*) AS OPERATION_HOURS -- Counts number of hourly records\nFROM HOURLY_RESORT_SUMMARY\nGROUP BY RIDE_DATE, RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb3c4734-4c15-47b4-a62e-d103c26bb9a4",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_DAILY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "select * from DAILY_RESORT_SUMMARY \norder by RIDE_DATE desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c9bd052-aaa1-41ec-b0b7-3298fc7d188a",
   "metadata": {
    "name": "MD_DT_WEEKLY_AGGREGATES_SETUP"
   },
   "source": "### 6.3. Weekly Aggregations\n\nThis Dynamic Table aggregates daily summaries to provide weekly insights."
  },
  {
   "cell_type": "code",
   "id": "a2675295-9549-45c0-b12d-8280c916a94d",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_WEEKLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "-- ========================================\n-- WEEKLY RESORT SUMMARY DYNAMIC TABLE\n-- Weekly aggregation from daily DT\n-- ========================================\nCREATE OR REPLACE DYNAMIC TABLE WEEKLY_RESORT_SUMMARY\nTARGET_LAG = '1 minute'\nWAREHOUSE = STREAMING_INGEST\nREFRESH_MODE = INCREMENTAL\nAS\nSELECT\n    DATE_TRUNC('week', RIDE_DATE) AS WEEK_START_DATE,\n    RESORT,\n    MAX(TOTAL_VISITORS) AS MAX_DAILY_UNIQUE_VISITORS, -- Peak unique visitors on any single day in the week\n    ROUND(AVG(TOTAL_VISITORS), 0) AS AVG_DAILY_UNIQUE_VISITORS, -- Average daily unique visitors\n    SUM(TOTAL_VISITORS) AS WEEK_TOTAL_VISITORS, -- Sum of daily unique visitors (visitor-days)\n    SUM(TOTAL_RIDES) AS WEEK_TOTAL_RIDES,\n    SUM(TOTAL_PASS_RIDES) AS WEEK_TOTAL_PASS_RIDES,\n    SUM(TOTAL_TICKET_RIDES) AS WEEK_TOTAL_TICKET_RIDES,     \n    SUM(TOTAL_TICKET_REVENUE) AS WEEK_TOTAL_TICKET_REVENUE,\n    SUM(TOTAL_PASS_REVENUE) AS WEEK_TOTAL_PASS_REVENUE,\n    SUM(TOTAL_REVENUE) AS WEEK_TOTAL_REVENUE,\n    ROUND(AVG(TOTAL_REVENUE), 0) AS AVG_DAILY_REVENUE,    \n    SUM(TOTAL_TICKET_ACTIVATIONS) AS WEEK_TOTAL_TICKET_ACTIVATIONS,\n    SUM(TOTAL_PASS_ACTIVATIONS) AS WEEK_TOTAL_PASS_ACTIVATIONS,    \n    ROUND(AVG(AVG_CAPACITY_PCT), 1) AS AVG_WEEK_CAPACITY_PCT, -- Average of the daily average capacities\n    MAX(PEAK_CAPACITY_PCT) AS WEEK_PEAK_CAPACITY_PCT,       -- Peak hourly capacity reached during the week\n    COUNT(DISTINCT RIDE_DATE) AS OPERATION_DAYS -- Count of distinct days with operations in the week\nFROM DAILY_RESORT_SUMMARY\nGROUP BY DATE_TRUNC('week', RIDE_DATE), RESORT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "459123e9-b822-4868-b41c-0fdc87ece572",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_DT_WEEKLY_RESORT_SUMMARY"
   },
   "outputs": [],
   "source": "select * from WEEKLY_RESORT_SUMMARY \norder by WEEK_START_DATE desc\nlimit 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "070dc4ea-a35e-4b29-8c13-446e33142848",
   "metadata": {
    "name": "MD_ANALYTICAL_VIEWS_SETUP",
    "collapsed": false
   },
   "source": "## 7. Analytical Views for Reporting\n\nCreate views on top of base tables and/or dynamic tables for easier querying and dashboarding."
  },
  {
   "cell_type": "code",
   "id": "5c851e66-e5fe-4e08-9100-043acfc0923f",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_RT_LIFT_PERFORMANCE"
   },
   "outputs": [],
   "source": "-- ========================================\n-- VIEW: V_RT_LIFT_PERFORMANCE\n-- Real-time lift performance based on last 30 minutes of activity from LIFT_RIDE table\n-- ========================================\nCREATE OR REPLACE VIEW V_RT_LIFT_PERFORMANCE AS\nWITH simulation_clock AS (\n    -- Determine the latest ride time to simulate a 'current time' for simulated data\n    SELECT\n        MAX(RIDE_TIME) as MAX_RIDE_TIME\n    FROM LIFT_RIDE\n),\nrecent_activity AS (\nSELECT\n    lr.RESORT,\n    lr.LIFT,\n    COUNT(*) as RIDES_30MIN,\n    COUNT(DISTINCT lr.RFID) as VISITORS_30MIN,\n    MAX(lr.RIDE_TIME) as LAST_ACTIVITY_TIME\nFROM LIFT_RIDE lr\n    CROSS JOIN simulation_clock clock\nWHERE lr.RIDE_TIME >= DATEADD(MINUTE, -30, clock.MAX_RIDE_TIME)\nGROUP BY lr.RESORT, lr.LIFT\n)\nSELECT\n    ra.RESORT,\n    ra.LIFT,\n    ra.RIDES_30MIN,\n    ra.VISITORS_30MIN,\n    ra.LAST_ACTIVITY_TIME,\n    ROUND(ra.RIDES_30MIN * 2.0, 1) as ESTIMATED_RIDES_PER_HOUR,\n    ROW_NUMBER() OVER (PARTITION BY ra.RESORT ORDER BY ra.RIDES_30MIN DESC) as USAGE_RANK_IN_RESORT\nFROM recent_activity ra\nORDER BY ra.RESORT, ra.RIDES_30MIN DESC;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "33d96bca-4e1f-47ab-8008-bd35ef5ee7aa",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_RT_LIFT_PERFORMANCE"
   },
   "outputs": [],
   "source": "select * from V_RT_LIFT_PERFORMANCE\norder by RESORT, LIFT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d50cbf4-0c8d-4950-86cb-114990437ac9",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_DAILY_REVENUE_PERFORMANCE",
    "codeCollapsed": false
   },
   "source": "-- ========================================\n-- VIEW: V_DAILY_REVENUE_PERFORMANCE\n-- Daily revenue vs targets, derived from DAILY_RESORT_SUMMARY and RESORT_CAPACITY\n-- ========================================\nCREATE OR REPLACE VIEW V_DAILY_REVENUE_PERFORMANCE AS\nWITH daily_targets AS (\n    SELECT\n        RESORT,\n        (MAX_CAPACITY * 0.7 * 100) as REVENUE_TARGET_USD -- Example target: 70% of max capacity value, assuming $100 per visitor\n    FROM RESORT_CAPACITY\n)\nSELECT\n    d.RIDE_DATE,\n    d.RESORT,\n    d.TOTAL_REVENUE,\n    t.REVENUE_TARGET_USD,\n    CASE\n        WHEN t.REVENUE_TARGET_USD > 0 THEN ROUND((d.TOTAL_REVENUE / t.REVENUE_TARGET_USD * 100), 1)\n        ELSE NULL\n        END as REVENUE_TARGET_PCT,\n    CASE\n        WHEN d.TOTAL_REVENUE >= t.REVENUE_TARGET_USD THEN 'ABOVE_TARGET'\n        WHEN d.TOTAL_REVENUE >= t.REVENUE_TARGET_USD * 0.9 THEN 'NEAR_TARGET'\n        ELSE 'BELOW_TARGET'\n        END as PERFORMANCE_STATUS\nFROM DAILY_RESORT_SUMMARY d\n         JOIN daily_targets t ON d.RESORT = t.RESORT;",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "83387ed7-764c-4700-8647-95f1dd82ce73",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_VIEW_DAILY_REVENUE_PERFORMANCE"
   },
   "outputs": [],
   "source": "select * from V_DAILY_REVENUE_PERFORMANCE order by RIDE_DATE DESC LIMIT 100;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d204a4a7-a078-4a57-bd32-6049b558590e",
   "metadata": {
    "language": "sql",
    "name": "SQL_VIEW_DAILY_NETWORK_METRICS"
   },
   "outputs": [],
   "source": "-- ========================================\n-- VIEW: V_DAILY_NETWORK_METRICS\n-- Simplified network-wide metrics for dashboard, derived from DAILY_RESORT_SUMMARY\n-- ========================================\nCREATE OR REPLACE VIEW V_DAILY_NETWORK_METRICS AS\nSELECT \n    RIDE_DATE,\n    SUM(TOTAL_VISITORS) as TOTAL_NETWORK_VISITORS,\n    SUM(TOTAL_REVENUE) as TOTAL_NETWORK_REVENUE,\n    ROUND(AVG(AVG_CAPACITY_PCT), 1) as AVG_NETWORK_CAPACITY_PCT, -- Average of average daily capacities\n    SUM(TOTAL_RIDES) as TOTAL_NETWORK_RIDES,\n    COUNT(DISTINCT RESORT) as ACTIVE_RESORTS\nFROM DAILY_RESORT_SUMMARY\nGROUP BY RIDE_DATE;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7906f19d-88eb-4a4c-b584-3e96ee251a0a",
   "metadata": {
    "language": "sql",
    "name": "SQL_QUERY_VIEW_DAILY_NETWORK_METRICS"
   },
   "outputs": [],
   "source": "select * from V_DAILY_NETWORK_METRICS order by RIDE_DATE DESC LIMIT 100;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b32f2a2e-6c1e-4f99-93ed-8e031146ef04",
   "metadata": {
    "name": "MD_SCHEMA_VERIFICATION_COMMANDS"
   },
   "source": "## 9. Schema Verification\n\nShow tables and views to verify the created objects."
  },
  {
   "cell_type": "code",
   "id": "7e50eff0-167e-4bcc-81c8-08a85b230954",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_BASE_TABLES"
   },
   "outputs": [],
   "source": "-- List base tables\nSHOW TABLES;\nSELECT * FROM TABLE(RESULT_SCAN(LAST_QUERY_ID())) WHERE \"is_dynamic\" = 'N';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c751954-96da-44ea-a539-789d2d8559c3",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_DYNAMIC_TABLES"
   },
   "outputs": [],
   "source": "-- List dynamic tables\nSHOW DYNAMIC TABLES;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5339cfb1-9c79-4fc7-a108-be7c53650e13",
   "metadata": {
    "language": "sql",
    "name": "SQL_SHOW_VIEWS_VERIFICATION"
   },
   "outputs": [],
   "source": "-- List views\nSHOW VIEWS;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0622d836-05dc-4c0b-b3b2-c46b990cab9c",
   "metadata": {
    "name": "MD_DYNAMIC_TABLE_OBSERVABILITY_QUERIES"
   },
   "source": "## 10. Dynamic Table Observability\n\nMonitor the health, refresh history, and status of your Dynamic Tables."
  },
  {
   "cell_type": "code",
   "id": "c7661178-733c-49f5-bba0-e98dca82d63c",
   "metadata": {
    "language": "sql",
    "name": "SQL_DT_REFRESH_HISTORY_MONITORING"
   },
   "outputs": [],
   "source": "-- Check refresh history for performance monitoring\nSELECT *\nFROM TABLE(INFORMATION_SCHEMA.DYNAMIC_TABLE_REFRESH_HISTORY(NAME_PREFIX => 'STREAMING_INGEST.STREAMING_INGEST.'))\nORDER BY refresh_start_time DESC;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ae30208-baad-4b84-8365-283a628e0d83",
   "metadata": {
    "name": "MD_CONCLUSION_AND_NEXT_STEPS"
   },
   "source": "## 11. Conclusion and Next Steps\n\nThis notebook has established an end-to-end streaming data pipeline incorporating Snowpark for complex transformations (`DAILY_VISITS`) and a hierarchy of Dynamic Tables for efficient, incremental aggregations.\n\n**Key Features Implemented:**\n- Automated daily unique visitor tracking using a Snowpark procedure and Task.\n- Multi-level aggregation pipeline (Hourly → Daily → Weekly) using Dynamic Tables.\n- Analytical views for simplified reporting and dashboarding.\n- Observability queries for monitoring Dynamic Table performance and health.\n\n**Potential Next Steps:**\n- Build Streamlit applications or connect BI tools to these views and Dynamic Tables for visualization.\n- Extend the pipeline with more advanced analytics, such as anomaly detection or predictive modeling.\n- Implement alerting based on DT status or data quality checks."
  }
 ]
}