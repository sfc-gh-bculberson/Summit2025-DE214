def stream_data(pipe_name, fn_get_data, fn_delete_data):
    props = {
        "account": account_name,
        "user": user_name,
        "database": database_name,
        "schema": schema_name,
        "private_key": private_key,
        "ROWSET_DEV_VM_TEST_MODE": "false",
    }
    with closing(SnowflakeStreamingIngestClient(client_name, **props)) as client:
        logger.info("sending rows with batching")

        channel = client.open_channel(
            channel_name, database_name, schema_name, pipe_name
        )
        latest_committed_offset_token = channel.get_latest_committed_offset_token()
        if not latest_committed_offset_token:
            latest_committed_offset_token = 0
        while True:
            rows = fn_get_data(latest_committed_offset_token, BATCH_SIZE)
            if len(rows) > 0:
                nl_json = "\n".join([row[1] for row in rows])
                latest_committed_offset_token = rows[-1][0]
                channel.insert_rows(nl_json, offset_token=latest_committed_offset_token)
                current_committed_offset_token = (
                    channel.get_latest_committed_offset_token()
                )
                if current_committed_offset_token:
                    fn_delete_data(current_committed_offset_token)
